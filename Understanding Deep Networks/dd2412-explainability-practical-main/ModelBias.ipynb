{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ba1913-0a60-4a86-99a1-566620e3f5f1",
   "metadata": {},
   "source": [
    "# Identifying model bias\n",
    "\n",
    "In this second part we will use explanation methods to identify a faulty classifier that was trained on biased data.\n",
    "Specifically, each image contains an artifact whose color is related to the class of the image.\n",
    "A model trained with such images will likely learn to disregard the image content entirely and only focus on\n",
    "the artifact to make a prediction.\n",
    "You will use one of the explanation methods implemented in the first part to spot the issue.\n",
    "\n",
    "Altough in this example the bias was introduced artificially, it's not uncommon to see this kind of telling artifacts\n",
    "in real-world datasets. For example, in a dataset of X-ray scans, one might find identifiers along the edge or\n",
    "marks left by doctors that could hinder the learning of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0e114-247e-4d62-b653-24567797b4c5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee41abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Requirement already satisfied: jax[cuda] in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (0.6.2)\n",
      "Requirement already satisfied: jaxlib<=0.6.2,>=0.6.2 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (0.6.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.26 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (1.26.4)\n",
      "Requirement already satisfied: opt_einsum in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (1.15.3)\n",
      "INFO: pip is looking at multiple versions of jax[cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (978 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (978 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.38-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "INFO: pip is still looking at multiple versions of jax[cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.36-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.35-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "  Using cached jaxlib-0.4.34-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.33-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.31-cp310-cp310-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.30-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.29-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.28-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.27-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.26-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.25-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.24-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.23-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.22-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.21-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.20-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.19-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.18-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.17-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.16-py3-none-any.whl.metadata (29 kB)\n",
      "  Using cached jax-0.4.14.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.13.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.12.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.11.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.10.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.9.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.8.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.7.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.6.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.5.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.4.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.3.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.2.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.1.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.25.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (4.15.0)\n",
      "  Using cached jax-0.3.24.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.23.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (2.3.1)\n",
      "Requirement already satisfied: etils[epath] in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax[cuda]) (1.13.0)\n",
      "  Using cached jax-0.3.22.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.21.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.20.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.19.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.17.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.16.tar.gz (1.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.15.tar.gz (1.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.14.tar.gz (990 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.13.tar.gz (951 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.12.tar.gz (947 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.11.tar.gz (947 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.10.tar.gz (939 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.9.tar.gz (937 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.8.tar.gz (935 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.7.tar.gz (944 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.6.tar.gz (936 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.5.tar.gz (946 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.4.tar.gz (924 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.3.tar.gz (924 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.2.tar.gz (926 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.1.tar.gz (912 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.3.0.tar.gz (896 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.2.28.tar.gz (887 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.2.27.tar.gz (873 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.2.26.tar.gz (850 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.2.25.tar.gz (786 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.2.24.tar.gz (786 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.2.22-py3-none-any.whl\n",
      "\u001b[33mWARNING: jax 0.2.22 does not provide the extra 'cuda'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: jax\n",
      "  Attempting uninstall: jax\n",
      "    Found existing installation: jax 0.6.2\n",
      "    Uninstalling jax-0.6.2:\n",
      "      Successfully uninstalled jax-0.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chex 0.1.90 requires jax>=0.4.27, but you have jax 0.2.22 which is incompatible.\n",
      "flax 0.10.7 requires jax>=0.6.0, but you have jax 0.2.22 which is incompatible.\n",
      "optax 0.2.6 requires jax>=0.5.3, but you have jax 0.2.22 which is incompatible.\n",
      "orbax-checkpoint 0.11.25 requires jax>=0.6.0, but you have jax 0.2.22 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jax-0.2.22\n",
      "Collecting git+https://github.com/n2cholas/jax-resnet.git\n",
      "  Cloning https://github.com/n2cholas/jax-resnet.git to /private/var/folders/mj/bwns80rs3psccqv0gz4hh8zc0000gn/T/pip-req-build-8vvkz9sb\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/n2cholas/jax-resnet.git /private/var/folders/mj/bwns80rs3psccqv0gz4hh8zc0000gn/T/pip-req-build-8vvkz9sb\n",
      "  Resolved https://github.com/n2cholas/jax-resnet.git to commit 5b00735aa0a68ec239af4a728ad4a596c1b551f6\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: flax in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (0.10.7)\n",
      "Requirement already satisfied: optax in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (0.2.6)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (4.9.9)\n",
      "Requirement already satisfied: better_exceptions in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: jax in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax-resnet==0.0.4) (0.2.22)\n",
      "Requirement already satisfied: jaxlib in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax-resnet==0.0.4) (0.6.2)\n",
      "Collecting jax (from jax-resnet==0.0.4)\n",
      "  Using cached jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: msgpack in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from flax) (1.1.1)\n",
      "Requirement already satisfied: orbax-checkpoint in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from flax) (0.11.25)\n",
      "Requirement already satisfied: tensorstore in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from flax) (0.1.77)\n",
      "Requirement already satisfied: rich>=11.1 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from flax) (14.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from flax) (4.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from flax) (6.0.3)\n",
      "Requirement already satisfied: treescope>=0.1.7 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from flax) (0.1.10)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from optax) (2.3.1)\n",
      "Requirement already satisfied: chex>=0.1.87 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from optax) (0.1.90)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from optax) (1.26.4)\n",
      "Requirement already satisfied: dm-tree in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (4.2.1)\n",
      "Requirement already satisfied: promise in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (4.21.12)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (7.1.0)\n",
      "Requirement already satisfied: pyarrow in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (21.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (2.32.5)\n",
      "Requirement already satisfied: simple_parsing in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (1.17.2)\n",
      "Requirement already satisfied: termcolor in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (3.1.0)\n",
      "Requirement already satisfied: toml in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from tensorflow-datasets) (1.17.3)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from chex>=0.1.87->optax) (1.0.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2025.9.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.5.2)\n",
      "Requirement already satisfied: zipp in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.23.0)\n",
      "Requirement already satisfied: einops in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (0.8.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax->jax-resnet==0.0.4) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax->jax-resnet==0.0.4) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from jax->jax-resnet==0.0.4) (1.15.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from rich>=11.1->flax) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from rich>=11.1->flax) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
      "Requirement already satisfied: nest_asyncio in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: aiofiles in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from orbax-checkpoint->flax) (24.1.0)\n",
      "Requirement already satisfied: humanize in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from orbax-checkpoint->flax) (4.13.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from orbax-checkpoint->flax) (3.20.2)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /opt/homebrew/Caskroom/miniconda/base/envs/dd2412/lib/python3.10/site-packages (from simple_parsing->tensorflow-datasets) (0.17.0)\n",
      "Using cached jax-0.6.2-py3-none-any.whl (2.7 MB)\n",
      "Installing collected packages: jax\n",
      "  Attempting uninstall: jax\n",
      "    Found existing installation: jax 0.2.22\n",
      "    Uninstalling jax-0.2.22:\n",
      "      Successfully uninstalled jax-0.2.22\n",
      "Successfully installed jax-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install \"jax[cuda]\" -f 'https://storage.googleapis.com/jax-releases/jax_cuda_releases.html'\n",
    "\n",
    "!pip install \\\n",
    "  flax optax \\\n",
    "  'git+https://github.com/n2cholas/jax-resnet.git' \\\n",
    "  tensorflow-datasets \\\n",
    "  better_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebeedbb-16eb-4a17-893f-6eca05a03d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"WARNING\")\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import flax\n",
    "import flax.core\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax_resnet\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "import tqdm\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb00d90-7347-498c-960b-855c2d0474d0",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c2b07-a8e7-4abe-8d97-0a8bef25fb1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = (\n",
    "    \"tench\",\n",
    "    \"English springer\",\n",
    "    \"cassette player\",\n",
    "    \"chain saw\",\n",
    "    \"church\",\n",
    "    \"French horn\",\n",
    "    \"garbage truck\",\n",
    "    \"gas pump\",\n",
    "    \"golf ball\",\n",
    "    \"parachute\",\n",
    ")\n",
    "\n",
    "BIAS_COLORS = [\n",
    "    [0.1215, 0.4666, 0.7058],\n",
    "    [0.5490, 0.3372, 0.2941],\n",
    "    [1.0000, 0.4980, 0.0549],\n",
    "    [0.1725, 0.6274, 0.1725],\n",
    "    [0.8392, 0.1529, 0.1568],\n",
    "    [0.5803, 0.4039, 0.7411],\n",
    "    [0.8901, 0.4666, 0.7607],\n",
    "    [0.7372, 0.7411, 0.1333],\n",
    "    [0.0901, 0.7450, 0.8117],\n",
    "    [0.4980, 0.4980, 0.4980],\n",
    "]\n",
    "\n",
    "RED = np.array([1.0, 0, 0])\n",
    "BLUE = np.array([0, 0, 1.0])\n",
    "\n",
    "\n",
    "def create_dataset(data_dir: str, batch_size: int):\n",
    "    ds_builder = tfds.builder(\"imagenette/320px-v2\", data_dir=data_dir)\n",
    "    ds_builder.download_and_prepare()\n",
    "\n",
    "    ds_val = ds_builder.as_dataset(\"validation\", as_supervised=True)\n",
    "    ds_val = ds_val.map(resize)\n",
    "    ds_val = ds_val.map(add_bias_pixel)\n",
    "    ds_val = ds_val.batch(batch_size)\n",
    "    ds_val = tfds.as_numpy(ds_val)\n",
    "\n",
    "    return ds_val\n",
    "\n",
    "\n",
    "def resize(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 224, 224)\n",
    "    return image / 255.0, label\n",
    "\n",
    "\n",
    "def add_bias_pixel(image, label):\n",
    "    hw_ = tf.reduce_sum(image, axis=[0, 1])\n",
    "    hw_ = tf.cast(hw_, tf.int32) % 30 + 140\n",
    "    h = hw_[0]\n",
    "    w = hw_[1]\n",
    "    color = tf.constant(BIAS_COLORS)[label]\n",
    "    mask = tf.meshgrid(tf.range(224), tf.range(224), indexing=\"ij\")\n",
    "    mask = (\n",
    "        (mask[0] % 12 != tf.cast(label, tf.int32) + 1)\n",
    "        & (mask[0] > h)\n",
    "        & (mask[0] < h + 12)\n",
    "        & (mask[1] % 5 < 2)\n",
    "        & (mask[1] > w)\n",
    "        & (mask[1] < w + 30)\n",
    "    )\n",
    "    image = tf.where(mask[:, :, None], color, image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    @jax.jit\n",
    "    def logits_fn(variables, img):\n",
    "        # img: [H, W, C], float32 in range [0, 1]\n",
    "        assert img.ndim == 3\n",
    "        img = normalize_for_resnet(img)\n",
    "        logits = model.apply(variables, img[None, ...], mutable=False)[0]\n",
    "        return logits.max(), logits\n",
    "\n",
    "    path = Path(path)\n",
    "    args = json.loads(Path.read_text(path / \"args.json\"))\n",
    "    variables_path = path / \"variables.npy\"\n",
    "\n",
    "    model = getattr(jax_resnet.resnet, f\"ResNet{args['resnet_size']}\")(n_classes=10)\n",
    "    variables = model.init(jax.random.PRNGKey(0), jnp.zeros((1, 224, 224, 3)))\n",
    "    variables = flax.serialization.from_bytes(variables, variables_path.read_bytes())\n",
    "\n",
    "    return logits_fn, variables\n",
    "\n",
    "\n",
    "def normalize_for_resnet(images):\n",
    "    # images: [..., H, W, 3], float32, range [0, 1]\n",
    "    mean = jnp.array([0.485, 0.456, 0.406])\n",
    "    std = jnp.array([0.229, 0.224, 0.225])\n",
    "    return (images - mean) / std\n",
    "\n",
    "\n",
    "def imagenet_to_imagenette_logits(logits):\n",
    "    \"\"\"Select the 10 imagenette classes from the 1000 imagenet classes.\"\"\"\n",
    "    return logits[..., [0, 217, 482, 491, 497, 566, 569, 571, 574, 701]]\n",
    "\n",
    "\n",
    "def show_images(images, labels=None, logits=None, ncols=4, width_one_img_inch=3.0):\n",
    "    B, H, W, *_ = images.shape\n",
    "    nrows = int(np.ceil(B / ncols))\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=width_one_img_inch * np.array([1, H / W]) * np.array([ncols, nrows]),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        squeeze=False,\n",
    "        facecolor=\"white\",\n",
    "    )\n",
    "    for b in range(B):\n",
    "        ax = axs.flat[b]\n",
    "        ax.imshow(images[b])\n",
    "        if labels is not None:\n",
    "            ax.set_title(CLASS_NAMES[labels[b]])\n",
    "        if logits is not None:\n",
    "            pred = logits[b].argmax()\n",
    "            prob = jax.nn.softmax(logits[b])[pred]\n",
    "            color = (\n",
    "                \"blue\" if labels is None else (\"green\" if labels[b] == pred else \"red\")\n",
    "            )\n",
    "            p = mpl.patches.Patch(color=color, label=f\"{prob:.2%} {CLASS_NAMES[pred]}\")\n",
    "            ax.legend(handles=[p])\n",
    "    fig.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def blend(a, b, alpha: float):\n",
    "    return (1 - alpha) * a + alpha * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279aaf9-4ca3-4268-a137-a499da9678ea",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "These tables summarize the hyperparameters used to train the models and their performance.\n",
    "\n",
    "Accuracy and loss are reported for two slightly different versions of the validation set:\n",
    "one that contains a clear source of bias and one that doesn't. If we only had access to \n",
    "the biased dataset and we did not know about the bias, we might be tempted to choose the \n",
    "first model, which achieves a much higher accuracy than the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26ae1d-3b8c-4903-a552-aad5950443ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [Path('output/biased'), Path('output/unbiased')]\n",
    "\n",
    "df_args = (\n",
    "    pd.DataFrame([json.loads(Path.read_text(p / \"args.json\")) for p in paths])\n",
    "    .drop(columns=\"output\")\n",
    "    .set_index(\"run_id\")\n",
    "    .sort_index()\n",
    ")\n",
    "display(df_args)\n",
    "\n",
    "df_test = pd.DataFrame(\n",
    "    [\n",
    "        {\"run_id\": p.parent.name, **json.loads(line)}\n",
    "        for p in paths\n",
    "        for line in Path.read_text(p / \"test.json\").splitlines()\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_test.pivot_table(\n",
    "        index=\"run_id\", columns=\"bias_pixel\", values=[\"accuracy\", \"loss\"]\n",
    "    )\n",
    "    .sort_index()\n",
    "    .style.format(\"{:.3f}\")\n",
    "    .format(\"{:.1%}\", subset=\"accuracy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce42c3b-2c09-4bb2-abb1-d28345c95326",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Reimplement one of the explanation methods from the previous notebook and use it to\n",
    "visualize the most important regions for the first few batches of images. \n",
    "\n",
    "- Can you spot the model that was trained on biased data?\n",
    "- Which explanation method did you choose? Can you motivate your choice?\n",
    "  Did you try others to see what worked best?\n",
    "- Can you summarize the explanation method and suggest why it works best here?\n",
    "\n",
    "Add your comments below:\n",
    "- **TODO**\n",
    "\n",
    "Note: `explanation_fn` will be called with `logits_fn`, `variables`, and `images`. Extra parameters can be put in `kwargs` and `partial` will take care of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1122e-a343-4ba5-9ed5-822946ca4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_fn(logits_fn, variables, img):\n",
    "    H, W, _ = img.shape\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    # logits: [num_classes]\n",
    "    # attrib: [H, W]\n",
    "    return logits, attrib\n",
    "\n",
    "\n",
    "kwargs = {}\n",
    "explanation_fn = partial(explanation_fn, **kwargs)\n",
    "explanation_fn = jax.vmap(explanation_fn, in_axes=(None, None, 0))\n",
    "explanation_fn = jax.jit(explanation_fn, static_argnames=[\"logits_fn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129df4e3-bf86-43e1-abf3-6bf58b278b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = create_dataset(\".\", batch_size=4)\n",
    "\n",
    "logits_fn_a, variables_a = load_checkpoint(\"output/biased\")\n",
    "logits_fn_b, variables_b = load_checkpoint(\"output/unbiased\")\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(ds_val):\n",
    "    display(Markdown(f\"## Batch {batch_idx}\"))\n",
    "\n",
    "    display(Markdown(f\"### Model A\"))\n",
    "    logits, relevance = explanation_fn(logits_fn_a, variables_a, images)\n",
    "    show_images(\n",
    "        blend(images, RED, relevance.clip(min=0)[..., None]),\n",
    "        labels,\n",
    "        logits,\n",
    "    )\n",
    "\n",
    "    display(Markdown(f\"### Model B\"))\n",
    "    logits, relevance = explanation_fn(logits_fn_b, variables_b, images)\n",
    "    show_images(\n",
    "        blend(images, RED, relevance.clip(min=0)[..., None]),\n",
    "        labels,\n",
    "        logits,\n",
    "    )\n",
    "\n",
    "    if batch_idx >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb4e15-05a9-4479-b828-1ba2b325d491",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "How long did it take you to complete this practical? This information is valuable to us to balance the difficulty of different practicals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fd2fb-14af-4853-ad2e-85271c8eff18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
