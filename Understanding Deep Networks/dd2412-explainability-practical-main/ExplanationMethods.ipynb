{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe5ddca",
   "metadata": {},
   "source": [
    "# Explanation methods\n",
    "\n",
    "Deep learning models are becoming better and better at making predictions.\n",
    "As researchers, regulators, and users, we are also interested in asking additional questions. \n",
    "Namely, we would like to _explain_ a decision in terms of the input.\n",
    "Where in an image is a model focusing on?\n",
    "What cues is the prediction based on? Ddoes it match our expectation?\n",
    "Can the model be trusted?\n",
    "\n",
    "In this practical, we will explore popular methods for explaining decisions made by image classifiers:\n",
    "- Simple occlusion\n",
    "- Gradient norm\n",
    "- Gradient x input\n",
    "- GradCAM\n",
    "- Integrated gradients\n",
    "\n",
    "With a working implementation of each method, we will compare explanations qualitatively on a few sample images.\n",
    "\n",
    "Furthermore, we will evaluate the correctness of each method quantitatively using the _deletion score_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79a478",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"jax[cuda]\" -f 'https://storage.googleapis.com/jax-releases/jax_cuda_releases.html'\n",
    "\n",
    "!pip install \\\n",
    "  flax optax \\\n",
    "  'git+https://github.com/n2cholas/jax-resnet.git' \\\n",
    "  tensorflow-datasets \\\n",
    "  better_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba21725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"WARNING\")\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from typing import Sequence\n",
    "\n",
    "import flax.core\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax_resnet\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import tabulate\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "import tqdm\n",
    "from flax.training.train_state import TrainState\n",
    "from IPython.display import display\n",
    "from jax import jit, vmap\n",
    "\n",
    "RED = np.array([1.0, 0, 0])\n",
    "BLUE = np.array([0, 0, 1.0])\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def normalize_zero_one(x):\n",
    "    \"\"\"Normalize a vector between 0 and 1.\"\"\"\n",
    "    res = (x - x.min()) / (x.max() - x.min())\n",
    "    res = jnp.clip(res, a_min=0, a_max=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def normalize_max(x):\n",
    "    \"\"\"Normalize a vector between -1 and 1.\"\"\"\n",
    "    res = x / jnp.abs(x).max()\n",
    "    res = jnp.clip(res, a_min=-1, a_max=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def blend(a, b, alpha: float):\n",
    "    \"\"\"Blend two float-valued images\"\"\"\n",
    "    return (1 - alpha) * a + alpha * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd730e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For simplicity, we will use the small ImageNette dataset that contains 10 easy-to-classify categories from ImageNet.\n",
    "\n",
    "Here we load the dataset and show a few images that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f97d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"tench\",\n",
    "    \"English springer\",\n",
    "    \"cassette player\",\n",
    "    \"chain saw\",\n",
    "    \"church\",\n",
    "    \"French horn\",\n",
    "    \"garbage truck\",\n",
    "    \"gas pump\",\n",
    "    \"golf ball\",\n",
    "    \"parachute\",\n",
    "]\n",
    "\n",
    "\n",
    "def show_images(images, labels=None, logits=None, ncols=4, width_one_img_inch=3.0):\n",
    "    B, H, W, *_ = images.shape\n",
    "    nrows = int(np.ceil(B / ncols))\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=width_one_img_inch * np.array([1, H / W]) * np.array([ncols, nrows]),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        squeeze=False,\n",
    "        facecolor=\"white\",\n",
    "    )\n",
    "    for b in range(B):\n",
    "        ax = axs.flat[b]\n",
    "        ax.imshow(images[b])\n",
    "        if labels is not None:\n",
    "            ax.set_title(CLASS_NAMES[labels[b]])\n",
    "        if logits is not None:\n",
    "            pred = logits[b].argmax()\n",
    "            prob = nn.softmax(logits[b])[pred]\n",
    "            color = (\n",
    "                \"blue\" if labels is None else (\"green\" if labels[b] == pred else \"red\")\n",
    "            )\n",
    "            p = mpl.patches.Patch(color=color, label=f\"{prob:.2%} {CLASS_NAMES[pred]}\")\n",
    "            ax.legend(handles=[p])\n",
    "    fig.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def resize(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 224, 224)\n",
    "    return image / 255.0, label\n",
    "\n",
    "\n",
    "ds_builder = tfds.builder(\"imagenette/320px-v2\", data_dir=\".\")\n",
    "ds_builder.download_and_prepare()\n",
    "\n",
    "ds = ds_builder.as_dataset(split=\"train\", batch_size=None, as_supervised=True)\n",
    "ds = ds.map(resize)\n",
    "ds = ds.batch(8)\n",
    "ds = tfds.as_numpy(ds)\n",
    "viz_batch = next(iter(ds))\n",
    "\n",
    "images, labels = viz_batch\n",
    "show_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ff424",
   "metadata": {},
   "source": [
    "## Pretrained ResNet\n",
    "\n",
    "We will focus on a ResNet 18 model for the explanations which has been ported from PyTorch thanks to [this repo](https://github.com/n2cholas/jax-resnet).\n",
    "\n",
    "The simplest way to load and run a ResNet model using `jax_resnet` is:\n",
    "```python\n",
    "ResNet, variables = jax_resnet.pretrained_resnet(size)\n",
    "model = ResNet()\n",
    "img = jnp.zeros(224, 224, 3)                        # [H, W, C]\n",
    "logits = model.apply(variables, img[None, ...])[0]  # [1000]\n",
    "```\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Here we load a pre-trained model and prepare it for our purposes. \n",
    "We want the following:\n",
    "1. The function should operate on a single image instead of a batch. \n",
    "   Altough counterintuitive, this will make it easier to reason about explanations later\n",
    "   and is more in tune with the philosophy of jax.\n",
    "2. The function should take care of normalizing the image with mean `[0.485, 0.456, 0.406]` \n",
    "   and std `[0.229, 0.224, 0.225]` as done for the PyTorch models that this model was converted from.\n",
    "   Refer to [torchvision.transforms.Normalize](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize)\n",
    "   for an example.\n",
    "3. Select out of the 1000 ImageNet classes the 10 ImageNette classes that we are interested in.\n",
    "4. The function should return the largest element of the 10-dimensional logits vector,\n",
    "   since later on we'll often compute gradients of it. The full logits  vector should\n",
    "   also be returned for prediction and visualization purposes.\n",
    "\n",
    "Complete the function `logits_fn` returned by `load_resnet` so that it fullfills the requirements above.\n",
    "Upon executing the cell you should see 7/8 correct predictions with almost-certain confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723342a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet(size):\n",
    "    \"\"\"Load a resnet model and return resnet_logits_fn and its variables.\n",
    "\n",
    "    Returns:\n",
    "        logits_fn: a jitted function that given one image applies\n",
    "                   the resnet model and returns the max logit\n",
    "                   value and the logits vector\n",
    "        variables: resnet variables to use with logits_fn\n",
    "    \"\"\"\n",
    "\n",
    "    def logits_fn(variables, img):\n",
    "        # img: [H, W, C], float32 in range [0, 1]\n",
    "        assert img.ndim == 3\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "        # logits: [num_classes]\n",
    "        return logits.max(), logits\n",
    "\n",
    "\n",
    "    ResNet, variables = jax_resnet.pretrained_resnet(size)\n",
    "    model = ResNet()\n",
    "    logits_fn = jax.jit(logits_fn)\n",
    "    return logits_fn, variables\n",
    "\n",
    "\n",
    "def normalize_for_resnet(image):\n",
    "    mean = jnp.array([0.485, 0.456, 0.406])\n",
    "    std = jnp.array([0.229, 0.224, 0.225])\n",
    "    return (image - mean) / std\n",
    "\n",
    "\n",
    "def imagenet_to_imagenette_logits(logits):\n",
    "    \"\"\"Select the 10 imagenette classes from the 1000 imagenet classes.\"\"\"\n",
    "    return logits[..., [0, 217, 482, 491, 497, 566, 569, 571, 574, 701]]\n",
    "\n",
    "\n",
    "logits_fn, variables = load_resnet(size=18)\n",
    "images, labels = viz_batch\n",
    "_, logits = jax.vmap(logits_fn, (None, 0))(variables, images)\n",
    "assert logits.shape == (images.shape[0], len(CLASS_NAMES))\n",
    "show_images(images, labels, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968ae90",
   "metadata": {},
   "source": [
    "## Explanation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f87cf",
   "metadata": {},
   "source": [
    "### Occlusion\n",
    "\n",
    "The simplest explanation method consists in removing patches of the input image and measuring the effect on prediction confidence.\n",
    "Specifically, we want to measure the drop (or increase) in confidence in the predicted class between the original non-occluded image and an occluded version.\n",
    "\n",
    "We will use a single square patch of fixed size that is scanned over the entire image without overlap, altough it would be possible to\n",
    "come up with more advanced patterns of occlusion.\n",
    "\n",
    "#### Task 2\n",
    "\n",
    "Complete the function `prepare_occlusions` that takes in a single image of shape `[H, W, 3]`\n",
    "and outputs a batch of images of shape `[S, S, H, W, 3]` where the image at `[i, j]` contains \n",
    "a black patch of size `[H/S, W/S]` whose top-left corner is placed at `[i*H/S, j*W/S]`.\n",
    "\n",
    "Explained with a drawing:\n",
    "```\n",
    "imgs[i, j] =\n",
    "                j*W/S\n",
    "      ┌───────────┬────┬────┐\n",
    "      │           |    |    │\n",
    "      │           |    |    │\n",
    "i*H/S ├ ─ ─ ─ ─ ─ ┼────┤    │\n",
    "      │           │####│    │\n",
    "      │           │####│    │\n",
    "      ├ ─ ─ ─ ─ ─ ┴────┘    │\n",
    "      │                     │\n",
    "      │                     │\n",
    "      │                     │\n",
    "      │                     │\n",
    "      │                     │\n",
    "      └─────────────────────┘\n",
    "```\n",
    "\n",
    "Remember that in jax arrays can not be modified in-place.\n",
    "Use [`at[].set()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) instead:\n",
    "```python\n",
    "x[idx] = y  # Bad\n",
    "x = x.at[idx].set(y)  # Good\n",
    "```\n",
    "\n",
    "Once the missing lines in `prepare_occlusions` are filled in, visualize the resulting batch of partially-occluded images to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_occlusions(img, steps: int):\n",
    "    H, W, _ = img.shape\n",
    "    imgs = jnp.tile(img, (steps, steps, 1, 1, 1))\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    # imgs: [steps, steps, H, W, 3]\n",
    "    return imgs\n",
    "\n",
    "\n",
    "prepare_occlusions = jax.jit(prepare_occlusions, static_argnames=\"steps\")\n",
    "\n",
    "show_images(\n",
    "    prepare_occlusions(viz_batch[0][0], steps=3).reshape(-1, 224, 224, 3),\n",
    "    ncols=3,\n",
    "    width_one_img_inch=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436b3d6",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Using `prepare_occlusions` implemented above, complete the missing lines in `occlusion_fn` following to this pseudo-code:\n",
    "```python\n",
    "probs = f(img)\n",
    "idx = argmax(probs)\n",
    "imgs = prepare_occlusions(img)\n",
    "relevance[i, j] = f(img)[idx] - f(imgs[i, j])[idx]\n",
    "relevance = resize(relevance, img.shape)\n",
    "```\n",
    "\n",
    "With a working implementation, the code below will show positive and negative attributions for eight images.\n",
    "Positive attribution is shown as a red overlay, while negative attribution is shown in blue (almost invisible except for the last image).\n",
    "\n",
    "Note: `jit` and `vmap` take care of speeding up and vectorizing `occlusion_fn`\n",
    "so that it works on a batch of images.\n",
    "You will see them used as wrappers or decorators throughout the notebook.\n",
    "\n",
    "Tips: \n",
    "- you want to compute how much the probability of the original prediction drops, apply `softmax` to the output of `logits_fn` to get probabilities and select the right class with `idx`\n",
    "- apply vmap twice to `logits_fn` to vectorize it over the two extra axes added by `prepare_occlusions`, you don't need two nested for loops\n",
    "- use [`jax.image.resize`](https://jax.readthedocs.io/en/latest/_autosummary/jax.image.resize.html#jax.image.resize) with `method=\"bilinear\"` to resize the heatmap to the original size\n",
    "- use `normalize_max` to rescale the attributions to a range that works well with the visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebbc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusion_fn(logits_fn, variables, img, steps: int):\n",
    "    H, W, _ = img.shape\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # logits_orig: [num_classes]\n",
    "    # attrib:      [H, W]\n",
    "    return logits_orig, attrib\n",
    "\n",
    "\n",
    "occlusion_fn = jax.jit(occlusion_fn, static_argnames=[\"logits_fn\", \"steps\"])\n",
    "occlusion_fn = jax.vmap(occlusion_fn, in_axes=(None, None, 0, None))\n",
    "\n",
    "images, labels = viz_batch\n",
    "logits_fn, variables = load_resnet(size=18)\n",
    "logits, relevance = occlusion_fn(logits_fn, variables, images, 6)\n",
    "\n",
    "images = blend(images, RED, jnp.clip(relevance, a_min=0)[..., None])\n",
    "images = blend(images, BLUE, -jnp.clip(relevance, a_max=0)[..., None])\n",
    "show_images(images, labels, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b6899",
   "metadata": {},
   "source": [
    "### Grad norm (sensitivity)\n",
    "\n",
    "An important tool for decision explanation is the gradient of the prediction function with respect to the input variable evaluated at the input image.\n",
    "Intuitively, the gradient expresses how much a change in the input would affect the prediction (actually the pre-softmax confidence).\n",
    "By evaluating the gradient at the input image, we can estimate the relevance $R_i$ of each pixel $i$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X &\\in \\mathbb{R}^{D} \\\\\n",
    "p &= f(X) \\\\\n",
    "R_i &= \\nabla f(X_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since our models operate on images, the gradient w.r.t. an image will have shape `[H, W, 3]`.\n",
    "For ease of visualization, we will compute the norm of the gradient at each pixel location and visualize it as a heatmap of shape `[H, W]`.\n",
    "\n",
    "#### Task 4\n",
    "\n",
    "Complete the missing lines of `grad_norm_fn` so that given a single input image it returns the associated logits and the pixel-wise norm of the gradient of the most confident prediction.\n",
    "\n",
    "Also, since we want to overlay the explanation to the image, make sure to scale the results in the range `[0, 1]`.\n",
    "\n",
    "Tips:\n",
    "- The function `logits_fn` prepared in task 1 returns the maximum logit as its first return value.\n",
    "- In jax one can use [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.value_and_grad)\n",
    "  to decorate a function so that both the value and its gradient are returned. \n",
    "- The function `jax.value_and_grad` can also take an extra parameter `has_aux`\n",
    "  to indicate that the original function returns more than one value and that those extra values should be returned by the decorated function too. Example:\n",
    "  ```python\n",
    "  def foo(a, x):\n",
    "      y = jnp.exp(x**2) - jnp.sin(a @ x)\n",
    "      return y.sum(), y\n",
    "  \n",
    "  foo_vg = jax.value_and_grad(foo, argnums=1, has_aux=True)\n",
    "  (y_sum, y), grad_x = foo_vg(a, x)\n",
    "  ```\n",
    "- use `normalize_max` to rescale the attributions to a range that works well with the visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456776d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_norm_fn(logits_fn, variables, img):\n",
    "    H, W, _ = img.shape\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # logits: [num_classes]\n",
    "    # grad:   [H, W]\n",
    "    return logits, grad\n",
    "\n",
    "\n",
    "grad_norm_fn = jax.jit(grad_norm_fn, static_argnames=[\"logits_fn\"])\n",
    "grad_norm_fn = jax.vmap(grad_norm_fn, in_axes=(None, None, 0))\n",
    "\n",
    "images, labels = viz_batch\n",
    "logits_fn, variables = load_resnet(size=18)\n",
    "logits, relevance = grad_norm_fn(logits_fn, variables, images)\n",
    "\n",
    "show_images(\n",
    "    # images * relevance[..., None],\n",
    "    blend(images, RED, relevance[..., None]),\n",
    "    labels,\n",
    "    logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b5dc0",
   "metadata": {},
   "source": [
    "### Grad x input\n",
    "\n",
    "To increase the sharpness of the explanations it's possible to multiply the value of the gradient with the corresponding input.\n",
    "Intuitively, the gradient expresses the importance of a certain feature and is now rescaled by how much that feature is present.\n",
    "\n",
    "$$R_i = X_i \\cdot \\nabla f(X_i)$$\n",
    "\n",
    "#### Task 5\n",
    "\n",
    "Modify `grad_norm_fn` so that the gradient is multiplied with the image before computing the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da614459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_x_input_fn(logits_fn, variables, img):\n",
    "    H, W, _ = img.shape\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    # logits: [num_classes]\n",
    "    # grad:   [H, W]\n",
    "    return logits, grad\n",
    "\n",
    "\n",
    "grad_x_input_fn = jax.vmap(grad_x_input_fn, in_axes=(None, None, 0))\n",
    "grad_x_input_fn = jax.jit(grad_x_input_fn, static_argnames=[\"logits_fn\"])\n",
    "\n",
    "images, labels = viz_batch\n",
    "logits_fn, variables = load_resnet(size=18)\n",
    "logits, relevance = grad_x_input_fn(logits_fn, variables, images)\n",
    "\n",
    "show_images(\n",
    "    # images * relevance[..., None],\n",
    "    blend(images, RED, relevance[..., None]),\n",
    "    labels,\n",
    "    logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fcbbc",
   "metadata": {},
   "source": [
    "### Integrated gradients\n",
    "\n",
    "As observed, gradient-based explanations appear very noisy. This is because gradients can only describe what happens in the local neighborhood of the input image when a pixel is changed by a small quantity, therefore:\n",
    "- some pixels might be very important for the prediction (e.g. the color of a flower), but the local gradient might be saturated (e.g. different shades of yellow will all give the same confidence in \"sunflower\") and therefore those pixels will not be marked as relevant.\n",
    "- a small step in the direction of the gradient might increase the prediction confidence, but a slightly larger step might decrease it further and an even slighly larger step might increase it again.\n",
    "\n",
    "The integrated gradients method proposes to address this issue by aggregating gradients along a linear path between the input image and a baseline (usually black). By considering a path, the noise associated to local gradients is reduced. Also, using a baseline image allows to express the explanation in relative terms rather than absolute.\n",
    "\n",
    "By expressing the path as $\\gamma(\\alpha) = B + \\alpha(X-B)$, the method can be expressed as:\n",
    "$$\n",
    "R_i = \\int_0^1 \\frac{\\partial f(\\gamma(\\alpha))}{\\partial\\gamma_i(\\alpha)} \\frac{\\partial\\gamma_i(\\alpha)}{\\partial\\alpha} \\ d\\alpha.\n",
    "$$\n",
    "\n",
    "Which can be approximated as:\n",
    "$$\n",
    "R_i \\approx (X_i - B_i) \\frac{1}{M} \\sum_{m=1}^M \\frac{\\partial f\\left(B + m/M(X-B)\\right)}{\\partial X_i}.\n",
    "$$\n",
    "\n",
    "Where $B$ indicates the black baseline, $M$ is the number of steps for approximating the path integral.\n",
    "\n",
    "The function below computes all intermediate images between an input `img` and a black baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ffbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"steps\"])\n",
    "def prepare_integrated_gradients(img, steps: int):\n",
    "    assert img.ndim == 3\n",
    "    return img[None, :, :, :] * jnp.linspace(1, 0, num=steps)[:, None, None, None]\n",
    "\n",
    "\n",
    "image = viz_batch[0][0]\n",
    "images = prepare_integrated_gradients(image, steps=8).reshape(-1, 224, 224, 3)\n",
    "show_images(images, width_one_img_inch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cce30e",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "\n",
    "Complete the function `integrated_grad_fn` so that:\n",
    "- a single image is taken as input\n",
    "- a prediction is made on the input image to determine the predicted class\n",
    "- a batch of progressively darker images is prepared with `prepare_integrated_gradients`\n",
    "- for each image in the batch, the gradients of the logit at `idx` is computed\n",
    "- the path integral is approximated using a finite sum\n",
    "\n",
    "According to the official implementation, only positive attributions are considered and attributions are averaged per pixel.\n",
    "\n",
    "Tips:\n",
    "- Store the index of the most-confident prediction for the input image as `idx` because we need to refer to it when computing gradients\n",
    "- At each intermediate step you don't want the gradient `max_logit`, i.e.\n",
    "  the first output of `logits_fn`, which you would get from `grad(logits_fn)`.\n",
    "  Instead you want the gradient of the `idx`-th element of `logits`, i.e.\n",
    "  the second output. Define a local function or a lambda and call `grad` on that.\n",
    "- You don't need for loops, use `vmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49006681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_grad_fn(logits_fn, variables, img, steps: int):\n",
    "    H, W, _ = img.shape\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # logits: [num_classes]\n",
    "    # grads:  [H, W]\n",
    "    return logits, grads\n",
    "\n",
    "\n",
    "integrated_grad_fn = jax.jit(integrated_grad_fn, static_argnames=[\"logits_fn\", \"steps\"])\n",
    "integrated_grad_fn = jax.vmap(integrated_grad_fn, in_axes=(None, None, 0, None))\n",
    "\n",
    "images, labels = viz_batch\n",
    "logits_fn, variables = load_resnet(size=18)\n",
    "logits, relevance = integrated_grad_fn(logits_fn, variables, images, 25)\n",
    "\n",
    "show_images(\n",
    "    # images * relevance[..., None],\n",
    "    blend(images, RED, relevance[..., None]),\n",
    "    labels,\n",
    "    logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b999f",
   "metadata": {},
   "source": [
    "### GradCAM\n",
    "\n",
    "GradCAM decomposes the ResNet model in two blocks: a CNN backbone and a linear classifier, separated by global average pooling.\n",
    "$$\n",
    "\\begin{align}\n",
    "X &\\in \\mathbb{R}^{H\\times W\\times 3} \\\\\n",
    "A &= \\text{Backbone}(X) \\in \\mathbb{R}^{H'\\times W'\\times K}\\\\\n",
    "Y &= \\text{Linear}(\\text{GAP}(A)) \\in \\mathbb{R}^C\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The main idea is to consider the gradient of the activations before global average pooling and use them to rescale the intensity of the associated feature maps.\n",
    "Specifically, if the predicted class is $c$, the scaling factor for the $k$-th feature map is:\n",
    "$$\n",
    "\\alpha_c^k = \\frac{1}{H'W'} \\sum_i^{H'}\\sum_j^{W'} \\frac{\\partial Y^c}{\\partial A^k_{i,j}}\n",
    "$$\n",
    "\n",
    "The feature maps are then combined into a sized-down attribution as:\n",
    "$$\n",
    "R_c = \\text{ReLU}\\left( \\sum_k^K \\alpha_c^k A^k\\right) \\in \\mathbb{R}^{H'\\times W'}\n",
    "$$\n",
    "\n",
    "Finally, the relevance heatmap is resized to match the input image. Compared to the gradient-based methods above, GradCAM produces much smoother heatmaps thanks to this upsampling operation.\n",
    "\n",
    "#### Task 7\n",
    "\n",
    "Implement the missing parts of `grad_cam_fn`:\n",
    "- First, process the image through the backbone\n",
    "- Then, process the features through global average pooling the through the classifier.\n",
    "  Remember that you'll need the gradients w.r.t. these features.\n",
    "- Once you have the gradients, combine them with the features as indicated above and \n",
    "  resize the relevance to the same size of the input image.\n",
    "- As usual, return both the logits vector and the relevance matrix.\n",
    "\n",
    "Tips:\n",
    "- You will need to apply the backbone and the classifier separately. \n",
    "  The function `load_resnet_for_grad_cam` takes care of splitting the model and its variables for you.<br/>\n",
    "  They are returned as two dictionaries, both containing the keys `backbone` and `gap_cls`.\n",
    "- When performing complex sums and products [`jnp.einsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html)\n",
    "  can drastically simplify the amount of error-prone reshaping code required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c12271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_fn(fns, variables, img):\n",
    "    H, W, _ = img.shape\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    # logits: [num_classes]\n",
    "    # grad:   [H, W]\n",
    "    return logits, grad\n",
    "\n",
    "\n",
    "def load_resnet_for_grad_cam(size):\n",
    "    @jax.jit\n",
    "    def backbone_fn(variables, img):\n",
    "        # img:   [H, W, C], float32 in range [0, 1]\n",
    "        # feats: [h, w, c], float32\n",
    "        img = normalize_for_resnet(img)\n",
    "        feats = backbone.apply(variables, img[None, ...], mutable=False)[0]\n",
    "        return feats\n",
    "\n",
    "    @jax.jit\n",
    "    def gap_classifier_fn(variables, feats):\n",
    "        # feats:  [h, w, c], float32\n",
    "        # logit:  float32\n",
    "        # logits: [10], float32\n",
    "        logits = gap_classifier.apply(variables, feats[None, ...], mutable=False)[0]\n",
    "        logits = imagenet_to_imagenette_logits(logits)\n",
    "        return logits.max(), logits\n",
    "\n",
    "    ResNet, variables = jax_resnet.pretrained_resnet(size)\n",
    "    model = ResNet()\n",
    "\n",
    "    backbone = nn.Sequential(model.layers[:-2])\n",
    "    backbone_vars = jax_resnet.slice_variables(variables, start=0, end=-2)\n",
    "    gap_classifier = nn.Sequential(model.layers[-2:])\n",
    "    gap_classifier_vars = jax_resnet.slice_variables(variables, start=len(model.layers) - 2, end=None)\n",
    "    return (\n",
    "        flax.core.freeze({\"backbone\": backbone_fn, \"gap_cls\": gap_classifier_fn}),\n",
    "        flax.core.freeze({\"backbone\": backbone_vars, \"gap_cls\": gap_classifier_vars}),\n",
    "    )\n",
    "\n",
    "\n",
    "grad_cam_fn = jax.jit(grad_cam_fn, static_argnames=[\"fns\"])\n",
    "grad_cam_fn = jax.vmap(grad_cam_fn, in_axes=(None, None, 0))\n",
    "\n",
    "images, labels = viz_batch\n",
    "fns, variables = load_resnet_for_grad_cam(size=18)\n",
    "logits, relevance = grad_cam_fn(fns, variables, images)\n",
    "\n",
    "show_images(\n",
    "    images * relevance[..., None],\n",
    "    labels,\n",
    "    logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3f6e1",
   "metadata": {},
   "source": [
    "## Deletion score\n",
    "\n",
    "So far, we have evaluated the explanations qualitatively by drawing them as heatmaps over a few sample images.\n",
    "A common metric used to evaluate the _correctness_ of an explanation method quantitatively is the _deletion score_.\n",
    "It is computed by progressively removing pixels from an image in order of importance and measuring the corresponding drop in confidence.\n",
    "The behavior can be visualized on a plot that has the percentage of removed pixels on the horizontal axis and the prediction confidence on the vertical axis. \n",
    "\n",
    "Ideally, if the most-relevant pixels are actually important for the prediction, their removal should induce a sudden drop in confidence for that label.\n",
    "To summarize this idea with a number we can compute the area under the curve: a low area indicates a quick decline in confidence, hence a good explanation method. This value is denoted as _deletion score_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de763d1",
   "metadata": {},
   "source": [
    "### Single-image deletion score\n",
    "\n",
    "#### Task 8\n",
    "\n",
    "Implement a function `prepare_deletion` that given an image and the associated relevance prepares a batch of `steps` images.\n",
    "If we indicate the resulting batch as `imgs`, not that:\n",
    "- `imgs[0]` corresponds to the input image\n",
    "- `imgs[s]` is a copy of the input image with `s/(steps-1)` percent of black pixels\n",
    "- `imgs[-1]` is an all-black image\n",
    "- pixels are set to zero in order of relevance with the most-relevant ones first\n",
    "- if a pixel `(i, j)` is set to black at step `s` it will remain black in all subsequent steps, i.e. the images become progressively more black\n",
    "\n",
    "The code below samples a random relevance mask and shows the images resulting from `prepare_deletion` so that you can verify the implementation.\n",
    "Check that the first regions to become black are the ones with the highest relevance.\n",
    "\n",
    "Tips:\n",
    "- It's easier to reason about the flattened versions of the image and the relevance matrices, use `jnp.ndarray.flatten` and `jnp.unravel_index` to move back and forth between one and two dimensions\n",
    "- use `jnp.argsort` and `jnp.array_split` to sort and split the relevance, but be careful about sorting in ascending/descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_deletion(img, relevance, steps: int):\n",
    "    assert relevance.shape == img.shape[:2]\n",
    "    H, W, _ = img.shape\n",
    "    imgs = jnp.tile(img, (steps, 1, 1, 1))\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    # imgs: [steps, H, W, 3]\n",
    "    return imgs\n",
    "\n",
    "\n",
    "prepare_deletion = jax.jit(prepare_deletion, static_argnames=\"steps\")\n",
    "\n",
    "relevance = jax.random.uniform(jax.random.PRNGKey(42), (7, 7))\n",
    "relevance = jax.image.resize(relevance, (224, 224), method=\"bilinear\")\n",
    "relevance = normalize_zero_one(relevance)\n",
    "\n",
    "image = plt.get_cmap('viridis')(relevance)[..., :3]\n",
    "\n",
    "steps=8\n",
    "images = prepare_deletion(image, relevance, steps)\n",
    "assert images.shape == (steps, *image.shape)\n",
    "show_images(images, width_one_img_inch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7937cd2",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "\n",
    "Using the `prepare_deletion` function implemented above, complete the missing lines of `deletion_score_fn`:\n",
    "- The function takes as input an image, its relevance, and a number of steps\n",
    "- The function returns a vector of length `steps` containing the probabilities associated to the top-scoring class predicted by the model as more and more relevant pixels are removed\n",
    "- The function returns the original prediction `pred_orig` too\n",
    "\n",
    "The cell below contains a few lines of code for plotting the resulting curve and the associated score.\n",
    "You should see the confidence curve slowly decreasing to zero and eventually rising up slighlty.\n",
    "\n",
    "Tips:\n",
    "- The expected value for the area under the curve is `0.270`\n",
    "- You don't need for loops, use `vmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion_score_fn(logits_fn, variables, img, relevance, steps):\n",
    "    H, W, _ = img.shape\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    # probs: [steps]\n",
    "    # pred_orig: int\n",
    "    return probs, pred_orig\n",
    "\n",
    "\n",
    "deletion_score_fn = jax.jit(deletion_score_fn, static_argnames=[\"logits_fn\", \"steps\"])\n",
    "\n",
    "image = viz_batch[0][2]\n",
    "relevance = jax.random.uniform(jax.random.PRNGKey(42), (7, 7))\n",
    "relevance = jax.image.resize(relevance, image.shape[:2], method=\"bilinear\")\n",
    "relevance = normalize_zero_one(relevance)\n",
    "\n",
    "steps = 8\n",
    "logits_fn, variables = load_resnet(size=18)\n",
    "probs, pred = deletion_score_fn(logits_fn, variables, image, relevance, steps)\n",
    "auc = sklearn.metrics.auc(np.linspace(0,1,steps), probs)\n",
    "assert len(probs) == steps\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 4))\n",
    "ax.fill_between(np.linspace(0, 1, steps), probs)\n",
    "ax.set_ylabel(f\"Confidence for '{CLASS_NAMES[pred]}'\")\n",
    "ax.grid(axis=\"y\")\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "ax.set_xlabel(\"Pixels removed\")\n",
    "ax.set_title(f\"Deletion score: {auc:.3f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae0cbd",
   "metadata": {},
   "source": [
    "### Visualize deletion on some images\n",
    "\n",
    "The following code visualizes the explanations of all methods for the eight images used so far.\n",
    "On the right the deletion curve and its area is also plotted.\n",
    "\n",
    "#### Task 10\n",
    "\n",
    "No implementation required, just take a moment to compare the explanations:\n",
    "- How do they look side-by-side? \n",
    "  Does the deletion curve match what you see in the image?\n",
    "- Which one do you trust the most? Motivate your feeling!\n",
    "- Does your judgement correlate well with the deletion score?\n",
    "- Is there a method that is consistently better?\n",
    "\n",
    "Add your comments below:\n",
    "- **TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_fn, variables = load_resnet(size=18)\n",
    "logits_fns_gc, variables_gc = load_resnet_for_grad_cam(size=18)\n",
    "\n",
    "# Use lambdas instead of partial because vmap doesn't play well with kwargs\n",
    "all_methods = {\n",
    "    \"occlusion\": lambda images: occlusion_fn(logits_fn, variables, images, 6),\n",
    "    \"grad\": lambda images: grad_norm_fn(logits_fn, variables, images),\n",
    "    \"grad_x_input\": lambda images: grad_x_input_fn(logits_fn, variables, images),\n",
    "    \"grad_cam\": lambda images: grad_cam_fn(logits_fns_gc, variables_gc, images),\n",
    "    \"integrated_gradients\": lambda images: integrated_grad_fn(logits_fn, variables, images, 20),\n",
    "}\n",
    "\n",
    "# These logits are only needed for visualization\n",
    "images, labels = viz_batch\n",
    "_, logits = jax.vmap(logits_fn, (None, 0))(variables, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deletion_score_fn_vmap = jax.vmap(deletion_score_fn, in_axes=(None, None, 0, 0, None))\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    len(images),\n",
    "    len(all_methods) + 1,\n",
    "    figsize=((len(all_methods) + 3) * 3, len(images) * 3),\n",
    "    gridspec_kw={\"width_ratios\": len(all_methods) * [1] + [3], \"wspace\": 0.001},\n",
    ")\n",
    "\n",
    "# Write true/predicted class on the left side of each row\n",
    "for ax, lb, lg in zip(axs[:, 0], labels, logits):\n",
    "    ax.set_ylabel(f\"True {CLASS_NAMES[lb]}\\nPred {CLASS_NAMES[lg.argmax()]}\")\n",
    "\n",
    "# Column headers: method names + deletion curve\n",
    "for ax, method in zip(axs[0, :-1], all_methods.keys()):\n",
    "    ax.set_title(method)\n",
    "axs[0, -1].set_title(\"Deletion curve\")\n",
    "\n",
    "# Each explanation method gets its own column on the left and its own curve on the right\n",
    "for method_col, (method, method_fn) in zip(axs[:, :-1].T, all_methods.items()):\n",
    "    _, relevance = method_fn(images)\n",
    "    for ax, img, rel in zip(method_col, images, relevance):\n",
    "        ax.imshow(blend(img, RED, normalize_zero_one(rel)[..., None]))\n",
    "\n",
    "    probs, _ = deletion_score_fn_vmap(logits_fn, variables, images, relevance, 25)\n",
    "    for ax, p in zip(axs[:, -1], probs):\n",
    "        auc = sklearn.metrics.auc(np.linspace(0, 1, len(p)), p)\n",
    "        ax.plot(np.linspace(0, 1, len(p)), p, label=f\"{auc:.3f} {method}\")\n",
    "\n",
    "# Remove inner ticks for the image grid\n",
    "for ax in axs[:-1, :-1].flat:\n",
    "    ax.set_xticks([])\n",
    "for ax in axs[:, 1:-1].flat:\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Annotate right column on the rightmost edge\n",
    "for ax in axs[:, -1]:\n",
    "    axt = ax.twinx()\n",
    "    axt.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "    axt.set_ylabel(\"Confidence\")\n",
    "    axt.grid()\n",
    "    ax.set_yticks([])\n",
    "    ax.legend(loc=\"upper right\", framealpha=1.0)\n",
    "\n",
    "# Remove pixel percent ticks from right column, except at the bottom\n",
    "for ax in axs[:-1, -1]:\n",
    "    ax.set_xticklabels([])\n",
    "axs[-1, -1].xaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "axs[-1, -1].set_xlabel(\"Pixels removed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6f18c",
   "metadata": {},
   "source": [
    "### Average deletion score on entire dataset\n",
    "\n",
    "To better compare the explanation methods, we can compute the average deletion score across the entire dataset.\n",
    "This value should give us an indication of which method is best at identifying relevant pixels for a prediction.\n",
    "\n",
    "#### Task 11\n",
    "\n",
    "No implementation required, just consider the results of this evaluation:\n",
    "- Which method seems to be best?\n",
    "- Can you trust results with such a high standard deviation?\n",
    "- What can be the cause of it? Think both of how the metric is computed and \n",
    "  of how the content of an image might affect the score.\n",
    "\n",
    "Add your comments below:\n",
    "- **TODO**\n",
    "\n",
    "Warning: the following code might take a long time to run and/or run out of memory.\n",
    "For reference, on a single GPU with 10 GB of memory and batch size 32, the total time for all the loops is approximately 10 minutes.\n",
    "You may need to reduce the batch size or limit the number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "deletion_steps = 10\n",
    "\n",
    "avg_auc = []\n",
    "deletion_steps_arr = np.linspace(0, 1, deletion_steps)\n",
    "logits_fn, variables = load_resnet(size=18)\n",
    "fns_gc, variables_gc = load_resnet_for_grad_cam(size=18)\n",
    "\n",
    "ds = ds_builder.as_dataset(split=\"train\", batch_size=None, as_supervised=True)\n",
    "ds = ds.map(resize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=True)\n",
    "ds = ds.batch(32, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE, deterministic=True)\n",
    "# ds = ds.take(50)  # limit number of batches\n",
    "ds = ds.prefetch(4)\n",
    "ds = tfds.as_numpy(ds)\n",
    "\n",
    "for method, method_fn in all_methods.items():\n",
    "    aucs = []\n",
    "    for images, labels in tqdm.tqdm(ds, ncols=0, desc=method):\n",
    "        _, relevance = method_fn(images)\n",
    "        probs, _ = deletion_score_fn_vmap(logits_fn, variables, images, relevance, deletion_steps)\n",
    "        aucs.extend(sklearn.metrics.auc(deletion_steps_arr, p) for p in probs)\n",
    "    avg_auc.append({\"method\": method, \"mean\": np.mean(aucs), \"std\": np.std(aucs)})\n",
    "    \n",
    "avg_auc = pd.DataFrame(avg_auc)\n",
    "display(avg_auc.set_index(\"method\"))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4), facecolor=\"white\")\n",
    "avg_auc.plot(\n",
    "    \"method\",\n",
    "    \"mean\",\n",
    "    yerr=\"std\",\n",
    "    kind=\"bar\",\n",
    "    rot=0,\n",
    "    figsize=(10, 5),\n",
    "    legend=None,\n",
    "    ylim=(0, 1),\n",
    "    xlabel=\"\",\n",
    "    title=\"Average deletion score (lower is better)\",\n",
    "    ax=ax\n",
    ")\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c46b9f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To help us improve this practical:\n",
    "- How long did it take to complete this notebook?\n",
    "- What was the most difficult part?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dd2412')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cf5542ead572ba171b6e63134e6b7d5858c565aa2759e72558e53d9a82af6bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
