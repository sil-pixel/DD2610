{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c1ad612-437c-4064-b35b-d20bd3dc23ef"
   },
   "source": [
    "# Part II - Layer Criticality\n",
    "\n",
    "In this part of the assignment, we will focus on reproducing an intriguing phenomenon of deep networks, known as *layer criticality*, first reported by [Zhang et al. (2019)](https://openreview.net/forum?id=ryg1P4Sh2E).\n",
    "\n",
    "The authors explore whether every layer of a discriminative deep network contributes equally to the performance of the learned classifier. Strikingly, it was observed that for some layers, the value of the corresponding parameters can be reset to initialization without severly affecting performance of the resulting model.\n",
    "\n",
    "In practice, starting from a trained network and the initialization checkpoint that was used for training, you will reproduce the main result of Zhang et al., for a pre-trained ResNet18 which was produced using the code from Part I of this assignment.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Let $\\theta^t = (\\theta_1^t, \\ldots, \\theta_L^t)$ denote the parameters of a $L$-layer network at epoch $t$, with $\\theta_\\ell^t$ representing the parameters of the $\\ell$-th layer. Let $t = T$ denote the final epoch of training. Consider now $\\theta^0 = \\theta^t_{|t = 0}$ &mdash; corresponding to the parameters of the same network at the specific initialization that was used for training.\n",
    "\n",
    "Let $\\mathcal{L}(\\theta^t, \\mathcal{D}) := \\mathbb{E}_{B_i \\sim \\mathcal{D}}{[}\\mathcal{L}(B_i, \\theta^t){]}$ denote the average loss of the network with parameters $\\theta^t$ over mini-batches $B_i$ sampled from a finite dataset $\\mathcal{D} = \\{(\\mathbf{x}_n, y_n) \\}_{n=1}^N$.\n",
    "\n",
    "To study the criticality of each layer $\\ell = 1, \\ldots, L$ for performance of the network, the parameter $\\theta^T = (\\theta_1^T, \\ldots, \\theta_\\ell^T, \\ldots, \\theta_L^T)$ is modified by setting $\\theta_\\ell^T = \\theta_\\ell^t$, for $t < T$. \n",
    "\n",
    "Then, the relative drop in performance is computed as:\n",
    "$$ drop = \\big(\\mathcal{L}(\\theta^T) - \\mathcal{L}(\\theta^T_{|\\theta_\\ell^T = \\theta_\\ell^t}) \\big) \\big/ \\mathcal{L}(\\theta^T),$$ \n",
    "where the dependency of $\\mathcal{L}$ on $\\mathcal{D}$ has been omitted to simplify notation.\n",
    "\n",
    "Given a list of checkpoint epochs $\\{t_1 = 0, \\ldots, t_s\\}$, the methodology proposed by Zhang et al. consists in iterating through the layers of a trained network $(t = T)$ and compute the drop in performance when a layer $\\ell$ is reset to its value at initialization, keeping all other layers fixed at their trained state.\n",
    "\n",
    "In this assigment, we will consider two choices of $\\mathcal{L}$, namely the cross-entropy loss and the 0/1 loss (accuracy). Finally, we will evaluate both losses on the train and test split of CIFAR-10.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "In order to solve this assignment, you will need to use the checkpoints of a pretrained ResNet18 following the model definition of Part I provided in memorization-part-II-ckpts.zip on Canvas. As previsouly noted, such model is a thinner version of ResNet v1 ([He et al., 2015](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html)), with fewer parameters to limit the computational requirements for this exercise. While the results reproduced in this assignment generalize to other datasets, we will use CIFAR-10 for simplicity ([Krizhevsky and Hinton, 2009](https://www.cs.toronto.edu/~kriz/cifar.html)).\n",
    "\n",
    "Note: it is a good idea to verify the integrity of the downloaded zip archive by matching its `sha256` hash with the one provided at the download link. Afterwards, extract all files from the archive and take note of the path where they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "326c0476-7c42-4401-bae1-01052e83ebd7"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import traverse_util\n",
    "import optax\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import PIL.Image\n",
    "\n",
    "from typing import Any, Callable, Sequence, Tuple, Dict\n",
    "from functools import partial\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, Markdown, display\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aecf2130-89ae-44a7-98df-6a8c31a31592"
   },
   "outputs": [],
   "source": [
    "os.environ['JAX_PLATFORM_NAME'] = 'gpu'\n",
    "# If running the notebook fails with OOM errors, please consider uncommenting the following line.\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
    "# If the notebook is failing at the very beginning with OOM error, uncomment the following line,\n",
    "# and tune the amout of preallocated memory according to your system.\n",
    "# os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.80'\n",
    "\"\"\"\n",
    "==============================\n",
    "TODO: Configuration required.\n",
    "==============================\n",
    "Edit below to set your path to CIFAR-10, as well as the target directory for \n",
    "loading checkpoints, and the path to the pre-trained model `resnet18_99.pickle`.\n",
    "The pre-trained model is provided as part of this assignment, together with \n",
    "checkpoints of several training epochs and initialization.\n",
    "\n",
    "We will assume the naming convention of models trained using the code from part\n",
    "I of this assignment. For example, `resnet18_EPOCH.pickle` will denote a \n",
    "resnet18 model checkpoint taken at training epoch EPOCH.\n",
    "\"\"\"\n",
    "data_dir = \"./data\" # path to cifar-10-batches-py\n",
    "checkpoints_dir = \"./checkpoints\"\n",
    "converged_model_fname = \"./checkpoints/resnet18_99.pickle\" # path to converged model checkpoint\n",
    "assert os.path.exists(converged_model_fname),\\\n",
    "    \"The path you provided does not exists!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62c2ddc6-367d-4985-915d-7a0f8277ac4a"
   },
   "outputs": [],
   "source": [
    "# Useful hyperparameters\n",
    "seed = 42\n",
    "enable_batch_norm = False\n",
    "batch_size = 128\n",
    "\n",
    "cifar10_mean = (0.4919, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
    "num_classes = 10\n",
    "\n",
    "\"\"\"\n",
    "==============================\n",
    "TODO: Configuration required.\n",
    "==============================\n",
    "Set the list of checkpoint epochs that will be used for the layer \n",
    "reinitialization experiments.\n",
    "The epochs should be picked by inspecting the checkpoint filenames of the \n",
    "provided pre-trained model.\n",
    "\n",
    "Keep in mind that training epochs are zero-indexed, and that a model \n",
    "initialization is saved using the special keyword 'init'.\n",
    "\n",
    "Your list of epochs should at least include checkpoints \"init\", and \"0\".\n",
    "\"\"\"\n",
    "raise NotImplementedError(\"Task: Implement!\")\n",
    "epochs = []\n",
    "\n",
    "# dictionary of epoch: checkpoint filename\n",
    "checkpoint_fnames = { \n",
    "    str(epoch): os.path.join(checkpoints_dir, f\"resnet18_{epoch}.pickle\") for epoch in epochs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dd4df89-20fc-440f-b37f-6b07a037751a"
   },
   "outputs": [],
   "source": [
    "# Data loading utilities\n",
    "def numpy_collate(batch):\n",
    "    \"\"\"Collate batch into a single numpy.ndarray\"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "class NumpyLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"Numpy dataloader subclassing pytorch's data loader\"\"\"\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                  shuffle=False, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0,\n",
    "                  pin_memory=False, drop_last=False,\n",
    "                  timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "# Transforms\n",
    "class ArrayNormalize(torch.nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, arr: np.ndarray) -> np.ndarray:\n",
    "        assert isinstance(arr, np.ndarray), \"Input should be ndarray, got {}.\".format(type(arr))\n",
    "        assert arr.ndim >= 3, \"Expected array to be image of size (*, H, W, C). Got {}.\".format(arr.shape)\n",
    "        \n",
    "        dtype = arr.dtype\n",
    "        mean = np.asarray(self.mean, dtype=dtype)\n",
    "        std = np.asarray(self.std, dtype=dtype)\n",
    "        if (std == 0).any():\n",
    "            raise ValueError(\"std evaluated to zero after conversion to {}\".format(dtype))\n",
    "        if mean.ndim == 1:\n",
    "            mean = mean.reshape(1, 1, -1)\n",
    "        if std.ndim == 1:\n",
    "            std = std.reshape(1, 1, -1)\n",
    "        arr -= mean\n",
    "        arr /= std\n",
    "        return arr\n",
    "\n",
    "\n",
    "class ToArray(torch.nn.Module):\n",
    "    dtype = np.float32\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        assert isinstance(x, PIL.Image.Image)\n",
    "        x = np.asarray(x, dtype=self.dtype)\n",
    "        x /= 255.0\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToArray(),\n",
    "    ArrayNormalize(cifar10_mean, cifar10_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53612af3-49b5-4fa3-9b89-94c304e7e702"
   },
   "outputs": [],
   "source": [
    "# load CIFAR-10\n",
    "cifar10_train = CIFAR10(data_dir, download=True, transform=transform, train=True)\n",
    "cifar10_test = CIFAR10(data_dir, download=True, transform=transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ce80a49-aaa8-43e4-820f-f2f113d590c1"
   },
   "outputs": [],
   "source": [
    "# set seed for reproducibilty\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "# instantiate data loaders\n",
    "train_loader = NumpyLoader(cifar10_train, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "test_loader = NumpyLoader(cifar10_test, batch_size=2*batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1SN3e9vqkmz"
   },
   "source": [
    "## ResNet model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00885277-c17a-466e-b807-b2c74f8c03f4"
   },
   "outputs": [],
   "source": [
    "ModuleDef = Any\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"ResNet basic block\"\"\"\n",
    "    filters: int\n",
    "    conv: ModuleDef\n",
    "    norm: ModuleDef\n",
    "    act: Callable\n",
    "    strides: Tuple[int,int] = (1,1)\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "        y = self.conv(self.filters, (3,3), self.strides)(x)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters, (3,3))(y)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
    "            \n",
    "        if residual.shape != y.shape:\n",
    "            residual = self.conv(self.filters, (1,1), self.strides, name='conv_proj')(residual)\n",
    "            if self.norm is not None:\n",
    "                residual = self.norm(name='norm_proj')(residual)\n",
    "                \n",
    "        return self.act(residual + y)\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"Bottleneck residual block\"\"\"\n",
    "    filters: int\n",
    "    conv: ModuleDef\n",
    "    norm: ModuleDef\n",
    "    act: Callable\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "        y = self.conv(self.filters, (1,1))(x)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters, (3, 3), self.strides)(y)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters * 4, (1,1))(y)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
    "            \n",
    "        if residual.shape != y.shape:\n",
    "            residual = self.conv(self.filters * 4, (1, 1), self.strides, name='conv_proj')(residual)\n",
    "            if self.norm is not None:\n",
    "                residual = self.norm(name='norm_proj')(residual)\n",
    "            \n",
    "        return self.act(residual + y)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet v1\"\"\"\n",
    "    stage_sizes: Sequence[int]\n",
    "    block_cls: ModuleDef\n",
    "    num_classes: int = 10 # adapted to CIFAR-10\n",
    "    num_filters: int = 16 # reduced number of filters to decrease training time\n",
    "    dtype: Any = jnp.float32\n",
    "    act: Callable = nn.relu\n",
    "    \n",
    "    def setup(self, enable_batch_norm=False):\n",
    "        self.enable_batch_norm = enable_batch_norm\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        conv = partial(nn.Conv, use_bias = not self.enable_batch_norm, dtype = self.dtype)\n",
    "        if self.enable_batch_norm:\n",
    "            norm = partial(nn.BatchNorm, use_running_average=not train, momentum=0.9, epsilon=1e-5, dtype=self.dtype)\n",
    "        else:\n",
    "            norm = None\n",
    "        \n",
    "        x = conv(self.num_filters, (3,3), (1,1),\n",
    "                 padding='SAME', name='conv_init')(x)\n",
    "        if self.enable_batch_norm:\n",
    "            x = norm(name='bn_init')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, (2, 2), strides=(2, 2), padding='SAME')\n",
    "        for i, block_size in enumerate(self.stage_sizes):\n",
    "            for j in range(block_size):\n",
    "                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n",
    "                x = self.block_cls(self.num_filters * 2 ** i,\n",
    "                                   strides=strides,\n",
    "                                   conv=conv,\n",
    "                                   norm=norm,\n",
    "                                   act=self.act)(x)\n",
    "        x = jnp.mean(x, axis=(1, 2))\n",
    "        x = nn.Dense(self.num_classes, dtype=self.dtype)(x)\n",
    "        x = jnp.asarray(x, self.dtype)\n",
    "        return x\n",
    "    \n",
    "ResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2], block_cls=ResNetBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-C_GKhVqhyy"
   },
   "source": [
    "## Evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f5c602b-7bd0-443e-9ae7-5de166b43579"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(logits, labels):\n",
    "    \"\"\"Given the model predictions @logits and ground-truth @labels compute\n",
    "       loss and accuracy.\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    Copy here your implementation of @compute_metrics from part I.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    metrics = {\n",
    "        'accuracy' : NotImplemented,\n",
    "        'loss' : NotImplemented\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    \"\"\"Evaluate ResNet18 on batch using params. \n",
    "    \"\"\"\n",
    "    logits = ResNet18().apply({'params': params}, batch['image'], train=False)\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])\n",
    "\n",
    "\n",
    "def eval_model(epoch, params, data_loader):\n",
    "    \"\"\"Evaluate model using data loader\n",
    "    \"\"\"\n",
    "    batch_metrics = []\n",
    "    for input, target in data_loader:\n",
    "        batch = {\n",
    "            'image': input,\n",
    "            'label': target,\n",
    "        }\n",
    "\n",
    "        metrics = eval_step(params, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "    }\n",
    "    split = \"train\" if data_loader.dataset.train else \"test\"\n",
    "    print(\"epoch: {}, {} loss: {}, {} accuracy: {}\".format(\n",
    "            epoch, split, epoch_metrics_np['loss'], split, epoch_metrics_np['accuracy']))\n",
    "    return epoch_metrics_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOJgLkGSqfNL"
   },
   "source": [
    "## Utilities for working with parameter dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xU8wtf4q4c8"
   },
   "outputs": [],
   "source": [
    "def load_state_dict(checkpoint_fname):\n",
    "    \"\"\"Load state dictionary from file.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(checkpoint_fname)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def load_checkpoint(rng, checkpoint_fname):\n",
    "    \"\"\"Restore saved checkpoint\n",
    "    \"\"\"\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "    params = net.init(rng, jnp.ones((1, 32, 32, 3)))['params']\n",
    "    state_dict = load_state_dict(checkpoint_fname)\n",
    "    \n",
    "    params = unfreeze(params)\n",
    "    params = state_dict[\"state\"]\n",
    "    params = freeze(params)\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def flatten_dict(d: Dict) -> Dict:\n",
    "    \"\"\"Traverse a nested dictionary @d and return a flattened version of it.\n",
    "    \"\"\"\n",
    "    return {'/'.join(k): v for k, v in traverse_util.flatten_dict(d).items()}\n",
    "\n",
    "\n",
    "def unflatten_dict(d: Dict) -> Dict:\n",
    "    \"\"\"Unflatten parameter dictionary @d back to a nested dictionary\n",
    "    \"\"\"\n",
    "    return traverse_util.unflatten_dict({tuple(k.split('/')): v for k, v in d.items()})\n",
    "\n",
    "\n",
    "def print_flat_params_dict(d: Dict) -> None:\n",
    "    \"\"\"Print flattened dictionary, together with the shape of all tensors it \n",
    "       contains\n",
    "    \"\"\"\n",
    "    print(jax.tree_map(jnp.shape, d))\n",
    "\n",
    "\n",
    "def layers(params: FrozenDict) -> str:\n",
    "    \"\"\"Yield each layer of a parameter dictionary\n",
    "    \"\"\"\n",
    "    def is_leaf(prefix, xs):\n",
    "        return 'kernel' in list(xs.keys())\n",
    "    \n",
    "    layer_ids = [ '/'.join(k) for k, _ in traverse_util.flatten_dict(unfreeze(params), is_leaf=is_leaf).items() ]\n",
    "    for layer in layer_ids:\n",
    "        yield layer\n",
    "\n",
    "\n",
    "def num_layers(params: FrozenDict) -> int:\n",
    "    \"\"\"Return the number of layers in a parameter dictionary\n",
    "    \"\"\"\n",
    "    return len(list(layers(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oq08N0lYq1xq"
   },
   "outputs": [],
   "source": [
    "def performance_drop(eval_metrics, metrics_converged):\n",
    "    \"\"\"Compute the performance drop of @eval_metrics relative to @metrics_converged\n",
    "       \n",
    "       ==============================\n",
    "       TODO: Implementation required.\n",
    "       ==============================\n",
    "       For each metric in @eval_metrics, compute the relative decrease in performance\n",
    "       from the corresponding metric in @metrics_converged.\n",
    "       \n",
    "       @return Dict: a dictionary {'accuracy': acc_drop, 'loss': loss_incr}\n",
    "               where acc_drop is the relative drop in accuracy computed by this function\n",
    "               and loss_incr is the relative loss increase.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    perf_drop = {\n",
    "        'accuracy' : NotImplemented,\n",
    "        'loss' : NotImplemented\n",
    "    }\n",
    "    \n",
    "    return perf_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CqVYVRlJKWo"
   },
   "source": [
    "test the `performance_drop` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl-XT9C6JK1O"
   },
   "outputs": [],
   "source": [
    "eval_metrics = {'accuracy': 60,\n",
    "                'loss' : 7.33}\n",
    "metrics_converged = {'accuracy': 99,\n",
    "                'loss' : 5.02}\n",
    "expected = {'accuracy': 0.3939393, \n",
    "            'loss': 0.4601593}\n",
    "output = performance_drop(eval_metrics, metrics_converged)\n",
    "for v1,v2 in zip(output.values(),\n",
    "                 expected.values()):\n",
    "    np.testing.assert_almost_equal(v1,v2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POikuS8crB8Q"
   },
   "outputs": [],
   "source": [
    "def update_metrics(epoch, metrics, new_metrics, split):\n",
    "    \"\"\"Update the dictionary metrics[split] with the keys and values\n",
    "       from @new_metrics. Return the updated dictionary.\n",
    "    \"\"\"\n",
    "    for key in new_metrics:\n",
    "        try:\n",
    "            metrics[split][key][str(epoch)].append(new_metrics[key].item())\n",
    "        except KeyError:\n",
    "            metrics[split][key][str(epoch)] = [new_metrics[key].item()]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCLjza9Aq_20"
   },
   "outputs": [],
   "source": [
    "def reinit_layer(params: FrozenDict, params_init: FrozenDict, layer_id: str) -> FrozenDict:\n",
    "    \"\"\"\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    Set @params at key @layer_id with the corresponding value from @params_init\n",
    "    \n",
    "    Hint: both weight and bias (if present) of layer @layer_id should be reinitialized\n",
    "    Hint: the FrozenDict params needs to be unfrozen and flattened for you to operate upon\n",
    "    Hint: the returned dictionary should be a FrozenDict\n",
    "    \n",
    "    @params: parameter dictionary of a trained model\n",
    "    @params_init: parameter dictionary of the same model, at initialization\n",
    "    @layer_id: dictionary key denoting the layer whose parameters are to be reinitialized. \n",
    "               The key should be chosen by inspecting the flattened dict @params.\n",
    "    \n",
    "    @return modified_params\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    modified_params = NotImplemented\n",
    "\n",
    "    return modified_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M401G20UJBHd"
   },
   "source": [
    "test the `reinit_layer` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b41aHu8pJAvp"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(seed)\n",
    "test_params = freeze(\n",
    "    {\"test_layer_0\":{\n",
    "        \"bias\": jnp.zeros(4),\n",
    "        \"kernel\": jnp.zeros((1,4)),\n",
    "        },\n",
    "     \"test_layer_1\": {\n",
    "        \"Conv_0\": {\n",
    "            \"bias\": jnp.zeros(6),\n",
    "            \"kernel\": jnp.zeros((4,6)),\n",
    "        },\n",
    "        \"Conv_1\": {\n",
    "            \"bias\": jnp.zeros(8),\n",
    "            \"kernel\": jnp.zeros((6,8)),\n",
    "        }\n",
    "        },\n",
    "     \"test_layer_2\":{\n",
    "        \"kernel\": jnp.zeros((8,4)),\n",
    "        },\n",
    "    })\n",
    "test_params_init = freeze(\n",
    "    {\"test_layer_0\":{\n",
    "        \"bias\": jnp.ones(4),\n",
    "        \"kernel\": jnp.ones((1,4)),\n",
    "        },\n",
    "     \"test_layer_1\": {\n",
    "        \"Conv_0\": {\n",
    "            \"bias\": jnp.ones(6),\n",
    "            \"kernel\": jnp.ones((4,6)),\n",
    "        },\n",
    "        \"Conv_1\": {\n",
    "            \"bias\": jnp.ones(8),\n",
    "            \"kernel\": jnp.ones((6,8)),\n",
    "        }\n",
    "        },\n",
    "     \"test_layer_2\":{\n",
    "        \"kernel\": jnp.ones((8,4)),\n",
    "        },\n",
    "    })\n",
    "\n",
    "test_params_flat = flatten_dict(test_params)\n",
    "test_params_init_flat = flatten_dict(test_params_init)\n",
    "layer_id = next(layers(test_params))\n",
    "returned = reinit_layer(test_params,\n",
    "                        test_params_init,\n",
    "                        layer_id)\n",
    "assert isinstance(returned,FrozenDict),\\\n",
    "    'the output of reinit_layer function should be of FrozenDict type'\n",
    "\n",
    "returned_flat = flatten_dict(returned)\n",
    "for layer in test_params_flat:\n",
    "    if layer_id in layer:\n",
    "        np.testing.assert_array_equal(\n",
    "            returned_flat[layer],\n",
    "            test_params_init_flat[layer])\n",
    "    else:\n",
    "        np.testing.assert_array_equal(\n",
    "            returned_flat[layer],\n",
    "            test_params_flat[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf825bfc-c5a5-430c-bacf-cd6f7ca20be2"
   },
   "outputs": [],
   "source": [
    "def reinit_layers(epoch, metrics, metrics_converged, params, params_epoch, train_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Loop through the layers of the @params dictionary. For each layer, \n",
    "    reinitialize the correponding entry in @params with that of @params_epoch\n",
    "    \n",
    "    Use @train_loader and @test_loader to evaluate the model resulting from \n",
    "    modifying @params.\n",
    "    \n",
    "    Compute the drop in performance of the modified model from the converged model\n",
    "    and store the result in @metrics using the key @epoch.\n",
    "    \"\"\"   \n",
    "    # loop through layers\n",
    "    for l in layers(params):\n",
    "        print(\"Reinitializing layer {}\".format(l))\n",
    "        params_reinit = reinit_layer(params, params_epoch, l)\n",
    "    \n",
    "        # eval network\n",
    "        eval_metrics = eval_model(epoch, params_reinit, test_loader)\n",
    "        train_metrics = eval_model(epoch, params_reinit, train_loader)\n",
    "        del params_reinit\n",
    "        \n",
    "        # performance drop\n",
    "        perf_drop_test = performance_drop(eval_metrics, metrics_converged['test'])\n",
    "        perf_drop_train = performance_drop(train_metrics, metrics_converged['train'])\n",
    "\n",
    "        # update metrics dictionary\n",
    "        metrics = update_metrics(epoch, metrics, perf_drop_test, split='test')\n",
    "        metrics = update_metrics(epoch, metrics, perf_drop_train, split='train')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv6LExOia1Hg"
   },
   "source": [
    "## main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db45fb34-1dd8-4bf0-bce3-23f44e33554d"
   },
   "outputs": [],
   "source": [
    "def reinit_network(checkpoint_fnames, converged_model_fname, train_loader, test_loader):\n",
    "    \"\"\"For each layer L and checkpoint C, reinitialize layer L to the parameters \n",
    "       values at checkpoint C and compute the relative drop in validation \n",
    "       performance.\n",
    "    \"\"\"\n",
    "    # Load checkpoint of converged model\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    params = load_checkpoint(rng, converged_model_fname)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'loss' : {},\n",
    "            'accuracy': {},\n",
    "        },\n",
    "        'test': {\n",
    "            'loss': {},\n",
    "            'accuracy': {},\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # dictionary of train/eval metrics for converged model\n",
    "    metrics_converged = {\n",
    "        'train' : { },\n",
    "        'test': { }\n",
    "    }\n",
    "    \n",
    "    # compute performance of converged model\n",
    "    print(\"Computing performance of converged model\")\n",
    "    metrics_converged['train'].update(\n",
    "        eval_model(99, params, test_loader))\n",
    "    metrics_converged['test'].update(\n",
    "        eval_model(99, params, train_loader))\n",
    "    \n",
    "    # iterate through checkpoint fnames\n",
    "    for epoch, checkpoint in checkpoint_fnames.items():\n",
    "        params_epoch = load_checkpoint(rng, checkpoint)\n",
    "        # iterate through layers\n",
    "        metrics = reinit_layers(\n",
    "            epoch, metrics, metrics_converged, params, params_epoch, train_loader, test_loader)\n",
    "        del params_epoch\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79d89c67-94e9-4cad-b019-7c293f3813b0"
   },
   "outputs": [],
   "source": [
    "metrics = reinit_network(checkpoint_fnames, converged_model_fname, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "489ce586-2400-437e-8a13-2bc8b3223ed1"
   },
   "source": [
    "## Criticality heatmaps\n",
    "\n",
    "Finally, we visualize the performance drop of reinitializing each layer at a specified epoch as a heatmap. By observing the plots below, answer the following.\n",
    "\n",
    "1. Which layers are most critical for performance?\n",
    "2. Can you see stronger impact on cross-entropy loss or 0/1 loss (accuracy)?\n",
    "3. If several layers can be reinitialized without considerable impact on performance, can you speculate on their role for training?\n",
    "\n",
    "Please note that layers are sorted in the order they are stored as a `FrozenDict`, rather than in the way they appear in the network. This is reflected in the criticality heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30effb60-d5ec-45ce-8b14-17bb7f00853f"
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "def get_layer_names():\n",
    "    params = load_checkpoint(jax.random.PRNGKey(seed), converged_model_fname)\n",
    "    names = list(layers(params))\n",
    "    return names\n",
    "    \n",
    "def to_matrix(results_dict):\n",
    "    \"\"\"Turn dictionary of lists into numpy matrix\"\"\"\n",
    "    matrix = np.asarray([ results_dict[key] for key in results_dict ], dtype=np.float64)\n",
    "    return matrix\n",
    "\n",
    "def plot_heatmap(crit_matrix, fname, cmap=None):\n",
    "    \"\"\"Plot heatmap given matrix. Save the figure to @fname\"\"\"\n",
    "    save_name = fname + '.png'\n",
    "    xlabels = get_layer_names()\n",
    "    ax = sns.heatmap(crit_matrix, xticklabels=xlabels, yticklabels=epochs, linewidth=0.5, cmap=cmap)\n",
    "    plt.savefig(save_name, bbox_inches=\"tight\", dpi=200)\n",
    "    plt.clf()\n",
    "    display(Markdown(f\"### {fname}\"))\n",
    "    display(Image(save_name))\n",
    "\n",
    "def plot_heatmaps(metrics, epochs, cmap=None):\n",
    "    \"\"\"Visualize each entry of @metrics as heatmap\"\"\"\n",
    "    def is_leaf(prefix, xs):\n",
    "        return epochs[0] in list(xs.keys())\n",
    "\n",
    "    flattened_metrics = traverse_util.flatten_dict(metrics, is_leaf=is_leaf)\n",
    "    for mname, m in flattened_metrics.items():\n",
    "        fname = '_'.join(mname)\n",
    "        crit_matrix = to_matrix(m)\n",
    "        plot_heatmap(crit_matrix, fname, cmap)\n",
    "        \n",
    "plot_heatmaps(metrics, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a050deba-cd76-44a5-b20a-ef9744279a8d"
   },
   "source": [
    "## Distance from initalization\n",
    "\n",
    "Can criticality be merely interpreted as the amount of non-zero gradient updates received by a layer, i.e. how much its parameters have changed from initialization? One way to explore this hypothesis is to compute a correlation coefficient between the distance of each parameter $\\theta_i^T$ from the corresponding parameter $\\theta_i^0$ at initialization, and the criticality scores stored in `metrics`.\n",
    "\n",
    "First, you are going to implement functions to compute the L2 distance of two parameters in the same weight space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uInTNKRdrHvk"
   },
   "outputs": [],
   "source": [
    "# compute l2 distance from initialization\n",
    "@jax.jit\n",
    "def distance_from_init_step(param1, param2):\n",
    "    \"\"\"Compute l2 distance of param1 from param2\n",
    "       ==============================\n",
    "       TODO: Implementation required.\n",
    "       ==============================\n",
    "       Hint: assume the parameter tensors are flattened\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    l2_dist = NotImplemented\n",
    "    \n",
    "    return l2_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77zSfZ-4O4qi"
   },
   "source": [
    "test the `distance_from_init_step` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4vE8rpHO465"
   },
   "outputs": [],
   "source": [
    "returned = distance_from_init_step(\n",
    "    test_params_flat['test_layer_0/kernel'], \n",
    "    test_params_init_flat['test_layer_0/kernel'])\n",
    "expected = 2.\n",
    "np.testing.assert_almost_equal(returned,\n",
    "                               expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8fPl2PGrKnU"
   },
   "outputs": [],
   "source": [
    "def distance_from_init(params: FrozenDict, params_init: FrozenDict, layer_id: str) -> float:\n",
    "    \"\"\"Compute L2 distance of @params from @params_init at @layer_id. This \n",
    "       function should ignore any bias parameter, and operate only on the \n",
    "       weight tensor.\n",
    "       \n",
    "       ==============================\n",
    "       TODO: Implementation required.\n",
    "       ==============================\n",
    "       \n",
    "       Hint: To operate on FrozenDicts you should perform analogous operations\n",
    "             as you did in reinit_layer\n",
    "       Hint: This function should call the JIT-optimized `distance_from_init_step`.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    l2_dist = NotImplemented\n",
    "    \n",
    "    return l2_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPh0osFhO_ey"
   },
   "source": [
    "test the `distance_from_init` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-Y5jPaAO_-e"
   },
   "outputs": [],
   "source": [
    "returned = distance_from_init(\n",
    "    test_params,\n",
    "    test_params_init,\n",
    "    \"test_layer_0\")\n",
    "expected = 2.0\n",
    "np.testing.assert_almost_equal(returned,\n",
    "                               expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMw0pYKnrTeW"
   },
   "outputs": [],
   "source": [
    "def reinit_layers(epoch, metrics, params, params_epoch):\n",
    "    \"\"\"\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    Modify the `reinit_layers` function to compute L2 distance of each weight \n",
    "    tensor in @params from the corresponding tensor in @params_epoch.\n",
    "    \n",
    "    Hint: to save GPU memory, after you are done with a modified parameter dictionary\n",
    "          you should mark it for garbage collection.\n",
    "    \"\"\"   \n",
    "    # loop through layers\n",
    "    for l in layers(params):\n",
    "        \n",
    "        raise NotImplementedError(\"Task: Implement!\")\n",
    "        l2_dist = NotImplemented\n",
    "\n",
    "        try:\n",
    "            metrics['l2_dist'][epoch].append(l2_dist)\n",
    "        except KeyError:\n",
    "            metrics['l2_dist'][epoch] = [l2_dist]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMrRoDVoscf0"
   },
   "source": [
    "test the `reinit_layers` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgsqTO1HscIE"
   },
   "outputs": [],
   "source": [
    "returned = reinit_layers(\"test\",\n",
    "                         {\"l2_dist\":\n",
    "                          {\"test\":\n",
    "                           []\n",
    "                           }},\n",
    "                         test_params,\n",
    "                         test_params_init)\n",
    "expected = np.array([2., \n",
    "                     4.8989791, \n",
    "                     6.9282031, \n",
    "                     5.6568541])\n",
    "np.testing.assert_array_almost_equal(returned[\"l2_dist\"][\"test\"],\n",
    "                              expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f819ec86-f100-405b-bed5-2c422cf0e654"
   },
   "outputs": [],
   "source": [
    "def distance_from_init_network(checkpoint_fnames, converged_model_fname, metrics):\n",
    "    \"\"\"For each layer L and checkpoint C, compute the L2 distance of the trained\n",
    "       parameters from those at the checkpoint C. Store the results in metrics.\n",
    "    \"\"\"\n",
    "    assert 'l2_dist' in metrics,\\\n",
    "        \"metrics should contain 'l2_dist' key.\"\n",
    "    # Load checkpoint of converged model\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    params = load_checkpoint(rng, converged_model_fname)\n",
    "    \n",
    "    # iterate through checkpoint fnames\n",
    "    for epoch, checkpoint in checkpoint_fnames.items():\n",
    "        params_epoch = load_checkpoint(rng, checkpoint)\n",
    "        # iterate through layers\n",
    "        metrics = reinit_layers(\n",
    "            epoch, metrics, params, params_epoch)\n",
    "        del params_epoch\n",
    "        \n",
    "    return metrics\n",
    "metrics['l2_dist'] = {}\n",
    "metrics = distance_from_init_network(checkpoint_fnames, converged_model_fname, metrics)\n",
    "l2_dist = {'l2_dist' : metrics['l2_dist']}\n",
    "plot_heatmaps(l2_dist, epochs, cmap=sns.color_palette(\"Blues\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92fab30b-ea20-473e-8352-c8fea3fb770d"
   },
   "source": [
    "### Spearman Correlation\n",
    "\n",
    "To conclude this exercise, you will compute the [Spearman rank correlation coeffiecient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) between the observed L2 distance $\\|\\theta^T_i - \\theta^0_i\\|_2$ for each layer, and the criticality values in `metrics` (train/test accuracy drop and loss increase).\n",
    "\n",
    "In general, a correlation coefficient very close to $1$ denotes strong correlation, while a value close to $-1$ denotes anti-correlation, and finally values around $0$ express no strong correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81c51a8e-cad8-4cfb-b69a-829200b9d75d"
   },
   "outputs": [],
   "source": [
    "def spearman_rank(metrics, epoch):\n",
    "    \"\"\"Compute the Spearman order correlation between the observed\n",
    "       L2 distances at @epoch and each metric in @metrics at @epoch\n",
    "       \n",
    "       ==============================\n",
    "       TODO: Implementation required.\n",
    "       ==============================\n",
    "       \n",
    "       Hint: for computing the Spearman rank, use `spearmanr` from `scipy`.\n",
    "             For this assignment, we are not interested in p_values, as\n",
    "             they are unreliable given the size of our observations.\n",
    "    \"\"\"\n",
    "    from scipy.stats import spearmanr\n",
    "\n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    l2_dist_correlation = NotImplemented\n",
    "    \n",
    "    return l2_dist_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeRQyaQ6W8iS"
   },
   "source": [
    "test the `spearman_rank` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96bkPmUQVQoI"
   },
   "outputs": [],
   "source": [
    "returned = spearman_rank({\n",
    "    \"test\": {\"init\":[3,3,2,1]},\n",
    "    \"l2_dist\": {\"init\": [1,2,3,4]}\n",
    "    },\n",
    "    'init')\n",
    "expected = -0.9486832\n",
    "np.testing.assert_almost_equal(returned['test'],\n",
    "                               expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDd3UnakXQia"
   },
   "source": [
    "\n",
    "What conclusions can you draw from the computed Spearman correlations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_exEVM9sVMch"
   },
   "outputs": [],
   "source": [
    "l2_dist_correlation = spearman_rank(metrics, 'init')\n",
    "print(\"Spearman correlations: {}\".format(l2_dist_correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is valuable for us to know how long did it take you to complete this practical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d16dda0b-9679-441d-aeb9-75bffbcf6ff5"
   },
   "source": [
    "## Optional: can non-critical layers be pruned?\n",
    "\n",
    "**Note.** This task is only provided for fun in case the practical has piqued your interest. Therefore, it is not part of the mandatory assignment and will not lead to a bonus either.\n",
    "As a optional task, repeat the criticality experiments on a VGG-11 network [Simonyan and Zisserman, 2015](https://www.robots.ox.ac.uk/~vgg/research/very_deep/) trained on CIFAR-10, using the code from Part I.\n",
    "\n",
    "More in detail, to complete the task you should:\n",
    "1. Extend the code of Part I to define a VGG-11 network (called 'config A' in the paper).\n",
    "2. Train VGG-11 on CIFAR-10, using data augmentation, no batch normalization, and on clean labels (no label noise). Save one checkpoint at initialization and for the trained model.\n",
    "3. Repeat the layer criticality experiment on VGG-11. What pattern do you observe in the criticality of its layers?\n",
    "4. Now go back to the code of Part I, and instantiate a new VGG model, by removing all non-critical layers. For this purpose, you can deem a layer non-critical if test accuracy drops at most by 0.1 when the layer is reset to its value at initialization.\n",
    "5. Train the newly defined model, with data augmentation and no batch norm. Can you match the same performance that you obtained with VGG-11?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d0ca219-aacc-4491-95c4-6451aecbd01a"
   },
   "source": [
    "## References\n",
    "* [Are All Layers Created Equal?](https://openreview.net/forum?id=ryg1P4Sh2E) - Zhang et al. ICML Workshop Deep Phenomena, 2019.\n",
    "* [Very Deep Convolutional Networks for Large-Scale Visual Recognition](https://www.robots.ox.ac.uk/~vgg/research/very_deep/) - Simonyan and Zisserman. ICRL 2015.\n",
    "* [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html) - He et al. ICCV 2015.\n",
    "* [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/cifar.html) - Krizhevsky and Hinton. 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5192440b-b96b-4d00-82cb-54330e627c1a"
   },
   "source": [
    "## Acknowledgements\n",
    "* The code for operating on parameter dictionaries is adapted from the official [Flax Model Surgery tutorial](https://flax.readthedocs.io/en/latest/howtos/model_surgery.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c79b2061-4110-40ad-8743-aad0a0300407"
   },
   "source": [
    "## Changelog\n",
    "| Version \t| Contribution      \t| Author (Affiliation) \t                | Contact \t        |\n",
    "|---------\t|-------------------\t|-----------------------------------    |---------\t        |\n",
    "| 1.0     \t| First development \t| Matteo Gamba (KTH/EECS/RPL)       \t|  ![contact address](figs/contact.png \"Contact information\") \t|"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
