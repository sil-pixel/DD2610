{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "560c8952-98a7-4819-8063-2409924ef142"
   },
   "source": [
    "# Part I - Deep neural networks can fit arbitrary labellings of the training data\n",
    "\n",
    "The mechanisms governing hypothesis selection in deep learning are currently not fully understood.\n",
    "\n",
    "According to traditional machine learning, a large hypothesis class (e.g. corresponding to overparameterized models) can fit several datasets of increasing complexity, which practically leads to overfitting training data and noise within. However, in practice, deep networks are able to achieve state-of-the-art performance on several benchmarks, thereby learning generalizing solutions. At the same time, weakly regularized networks can fit arbitrary labellings of the dataset, showing that stochastic optimization, coupled with overparameterization, can in principle memorize a dataset.\n",
    "\n",
    "A paramount question is thus what factors govern learning vs memorization, and why large, overparameterized networks don't simply memorize the training dataset?\n",
    "\n",
    "In this assignment, we are going to explore established phenomena that set deep networks aside from traditional machine learning, so that you familiarize yourself with some fundamental open questions in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abd4b59d-7f9e-4583-8140-5825bb85729b"
   },
   "source": [
    "![Hypothesis selection in deep learning?](figs/novak_et_al_implicit_reg.png \"Novak et al. Implicit regularization conjecture\")\n",
    "\n",
    "|                                                                         |\n",
    "| ----------------------------------------------------------------------: |\n",
    "| Source: [Novak et al. (2018)](https://openreview.net/forum?id=HJC2SzZCW) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71199180-dd2c-4775-98a6-f2cb8ed22acf"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In part I, we explore hypothesis selection in deep learning for image classification, by contrasting \"learning\" and \"memorization\".\n",
    "\n",
    "Basically, we examine the empirical behaviour of deep networks trained to perfectly fit noisy labels, as a way to force the model to represent complex decision boundaries. Further, we empirically demonstrate that large networks are capable of completely overfitting a dataset [Zhang et al. (2017)](https://arxiv.org/abs/1611.03530).\n",
    "\n",
    "Importantly, corrupting every training label disrupts the semantic relationship between input images and classes, forcing the network to learn decision boundaries tailored to each individual training sample. This will constitute our operative definition of high-complexity classifier *memorizing* the training set. Changing the ratio of noisy labels allows us to control complexity of our trained statistical model.\n",
    "\n",
    "In the first part of the assignment, we are going to implement the basic building blocks for our experiments, by reproducing the main result of [Zhang et al. (2017)](https://arxiv.org/abs/1611.03530).\n",
    "\n",
    "## Tasks\n",
    "\n",
    "Your tasks for this part of the assignment include:\n",
    "1. Corrupting labels of the CIFAR-10 training set.\n",
    "2. Training a small ResNet to fit corrupted labels.\n",
    "3. Discuss the effect of explicit regularization on memorization.\n",
    "4. Discuss implicit regularization in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fad6e7",
   "metadata": {
    "id": "e7d7f722-2326-489a-8b4e-fd3d687b267c"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import PIL.Image\n",
    "\n",
    "from typing import Any, Callable, Sequence, Tuple\n",
    "from functools import partial\n",
    "import random\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4571ea6-8fd1-4502-a005-712dfd9fef9b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['JAX_PLATFORM_NAME'] = 'gpu'\n",
    "# If running the notebook fails with OOM errors, please consider uncommenting the following line.\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
    "# If the notebook is failing at the very beginning with OOM errors, uncomment the following line,\n",
    "# and tune the amout of preallocated memory according to your system.\n",
    "# os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.80'\n",
    "\"\"\"\n",
    "==============================\n",
    "TODO: Configuration required.\n",
    "==============================\n",
    "Edit below to set your path to CIFAR-10 and the destination directory for saving checkpoints\n",
    "In case you don't have a local copy of the dataset, it will be downloaded automatically.\n",
    "\"\"\"\n",
    "data_dir = \"./data\" # path to cifar-10-batches-py\n",
    "checkpoints_dir = \"./checkpoints\"\n",
    "\n",
    "assert os.path.exists(checkpoints_dir),\\\n",
    "    \"The path you provided does not exitst!\"\\\n",
    "    \" Consider creating the folder manually.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c6177fc-44a1-4d2e-91ea-2ca9d8b867c4"
   },
   "source": [
    "The focus of this assignment is on *implicit regularization* [[Neyshabur, Tomioka, and Srebro (2015)](https://openreview.net/forum?id=6AzZb_7Qo0e)], namely that controlled by priors induced by the network architecture, optimizer, as well as the training data itself. Hence, we are going to disable all explicit regularization like weight decay, data augmentation, and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa178a90-413d-4b71-aac4-946ed78f7a94",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training hyperparameters\n",
    "==============================\n",
    "TODO: Configuration required.\n",
    "==============================\n",
    "Set the training hyperparameters by:\n",
    "1. Disabling explicit regularization\n",
    "2. Configuring the hyperparameters that control the learning rate schedule \n",
    "   described in Zhang et al. (2017). Specifically, a starting learning rate of \n",
    "   0.1 is decayed by a multiplicative factor of 0.95 at every epoch.\n",
    "3. To cap the computational cost of this assignment, we set a max number of epochs\n",
    "   to 100.\n",
    "4. In this assignment, we will explore how a deep network can fit arbitrary labelling of the data,\n",
    "   so we set the noise_ratio to 1.0\n",
    "\"\"\"\n",
    "cifar10_mean = (0.4919, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
    "num_classes = 10\n",
    "\n",
    "seed = 42\n",
    "enable_augmentation = # bool\n",
    "enable_batch_norm = # bool\n",
    "batch_size = 128# int\n",
    "noise_ratio = # float between 0. and 1.\n",
    "weight_decay = # float\n",
    "lr =  # float\n",
    "lr_decay =  # float\n",
    "max_epochs = # int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eac78af-ef79-4589-a53d-6fbd808910eb"
   },
   "outputs": [],
   "source": [
    "# Data loading utilities\n",
    "def numpy_collate(batch):\n",
    "    \"\"\"Collate batch into a single numpy.ndarray\"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "class NumpyLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"Numpy dataloader subclassing pytorch's data loader\"\"\"\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                  shuffle=False, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0,\n",
    "                  pin_memory=False, drop_last=False,\n",
    "                  timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "# Transforms\n",
    "class ArrayNormalize(torch.nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, arr: np.ndarray) -> np.ndarray:\n",
    "        assert isinstance(arr, np.ndarray), \"Input should be ndarray, got {}.\".format(type(arr))\n",
    "        assert arr.ndim >= 3, \"Expected array to be image of size (*, H, W, C). Got {}.\".format(arr.shape)\n",
    "        \n",
    "        dtype = arr.dtype\n",
    "        mean = np.asarray(self.mean, dtype=dtype)\n",
    "        std = np.asarray(self.std, dtype=dtype)\n",
    "        if (std == 0).any():\n",
    "            raise ValueError(\"std evaluated to zero after conversion to {}\".format(dtype))\n",
    "        if mean.ndim == 1:\n",
    "            mean = mean.reshape(1, 1, -1)\n",
    "        if std.ndim == 1:\n",
    "            std = std.reshape(1, 1, -1)\n",
    "        arr -= mean\n",
    "        arr /= std\n",
    "        return arr\n",
    "\n",
    "\n",
    "class ToArray(torch.nn.Module):\n",
    "    dtype = np.float32\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        assert isinstance(x, PIL.Image.Image)\n",
    "        x = np.asarray(x, dtype=self.dtype)\n",
    "        x /= 255.0\n",
    "        return x\n",
    "\n",
    "if enable_augmentation:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        ToArray(),\n",
    "        ArrayNormalize(cifar10_mean, cifar10_std)])\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        ToArray(),\n",
    "        ArrayNormalize(cifar10_mean, cifar10_std)])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    ToArray(),\n",
    "    ArrayNormalize(cifar10_mean, cifar10_std)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b62018c-d22d-48a4-9dc1-48a215e57928"
   },
   "outputs": [],
   "source": [
    "# load CIFAR-10\n",
    "cifar10_train = CIFAR10(data_dir, download=True, transform=transform_train, train=True)\n",
    "cifar10_test = CIFAR10(data_dir, download=True, transform=transform_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43899143-1e81-4701-a3bd-87f0f6d58251"
   },
   "outputs": [],
   "source": [
    "def corrupt_labels(dataset, noise_ratio, seed=36912):\n",
    "    \"\"\"\n",
    "    Corrupt dataset labels\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    * Replace a portion of the dataset labels (specificed by @noise_ratio) with random labels.\n",
    "      Random labels should be sampled uniformly, by also ensuring that the ground truth label is never\n",
    "      sampled. @noise_ratio is a float value between 0 and 1, denoting the ratio of corrupted labels.\n",
    "    \n",
    "    Conveniently, the skeleton provided loads the labels from @dataset and sets them after corruption.\n",
    "    A check is in place to test whether your implementation is correct.\n",
    "    \"\"\"\n",
    "    if noise_ratio == 0.:\n",
    "        return\n",
    "    assert 0. <= noise_ratio <= 1., \\\n",
    "        \"noise_ratio should be a value between 0 and 1. Got {}\".format(noise_ratio)\n",
    "    rng = jax.random.PRNGKey(seed) # set seed for reproducibility\n",
    "    labels = np.array(dataset.targets)\n",
    "    num_classes = len(dataset.classes)\n",
    "    \n",
    "    raise NotImplementedError('Task: implement!')\n",
    "    labels = NotImplemented\n",
    "\n",
    "    # set corrupted labels\n",
    "    dataset.targets = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_IyxcpaFkPo"
   },
   "source": [
    "Test the `corrupt_labels` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ATuqL8-Fby1"
   },
   "outputs": [],
   "source": [
    "# test how many labels were corrupted\n",
    "orig_labels = cifar10_train.targets.copy()\n",
    "\n",
    "corrupt_labels(cifar10_train, noise_ratio=noise_ratio)\n",
    "corrupted = np.float64(np.sum(orig_labels != cifar10_train.targets) / len(orig_labels))\n",
    "expected = np.float64(noise_ratio)\n",
    "np.testing.assert_allclose(corrupted, expected, verbose=True, rtol=1e-2,\n",
    "    err_msg=\"Effectively corrupted {}% labels, but {}% required\".format(corrupted *100, expected * 100))\n",
    "\n",
    "# undo the changes\n",
    "cifar10_train.targets = orig_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c6c0109-f2a1-495e-be79-3ca52aebb2aa"
   },
   "outputs": [],
   "source": [
    "# corrupt training labels\n",
    "corrupt_labels(cifar10_train, noise_ratio=noise_ratio)\n",
    "\n",
    "# set seed for reproducibilty\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "# instantiate data loaders\n",
    "train_loader = NumpyLoader(cifar10_train, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "test_loader = NumpyLoader(cifar10_test, batch_size=2*batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed55dc19-efa1-4e66-afff-836c6b7193a8"
   },
   "outputs": [],
   "source": [
    "# ResNet model definition\n",
    "ModuleDef = Any\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"ResNet basic block\"\"\"\n",
    "    filters: int\n",
    "    conv: ModuleDef\n",
    "    norm: ModuleDef\n",
    "    act: Callable\n",
    "    strides: Tuple[int,int] = (1,1)\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "        y = self.conv(self.filters, (3,3), self.strides)(x)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters, (3,3))(y)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
    "            \n",
    "        if residual.shape != y.shape:\n",
    "            residual = self.conv(self.filters, (1,1), self.strides, name='conv_proj')(residual)\n",
    "            if self.norm is not None:\n",
    "                residual = self.norm(name='norm_proj')(residual)\n",
    "                \n",
    "        return self.act(residual + y)\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"Bottleneck residual block\"\"\"\n",
    "    filters: int\n",
    "    conv: ModuleDef\n",
    "    norm: ModuleDef\n",
    "    act: Callable\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "        y = self.conv(self.filters, (1,1))(x)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters, (3, 3), self.strides)(y)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters * 4, (1,1))(y)\n",
    "        if self.norm is not None:\n",
    "            y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
    "            \n",
    "        if residual.shape != y.shape:\n",
    "            residual = self.conv(self.filters * 4, (1, 1), self.strides, name='conv_proj')(residual)\n",
    "            if self.norm is not None:\n",
    "                residual = self.norm(name='norm_proj')(residual)\n",
    "            \n",
    "        return self.act(residual + y)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet v1\"\"\"\n",
    "    stage_sizes: Sequence[int]\n",
    "    block_cls: ModuleDef\n",
    "    num_classes: int = 10 # adapted to CIFAR-10\n",
    "    num_filters: int = 16 # reduced number of filters to decrease training time\n",
    "    dtype: Any = jnp.float32\n",
    "    act: Callable = nn.relu\n",
    "    \n",
    "    def setup(self, enable_batch_norm=False):\n",
    "        self.enable_batch_norm = enable_batch_norm\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        conv = partial(nn.Conv, use_bias = not self.enable_batch_norm, dtype = self.dtype)\n",
    "        if self.enable_batch_norm:\n",
    "            norm = partial(nn.BatchNorm, use_running_average=not train, momentum=0.9, epsilon=1e-5, dtype=self.dtype)\n",
    "        else:\n",
    "            norm = None\n",
    "        \n",
    "        x = conv(self.num_filters, (3,3), (1,1),\n",
    "                 padding='SAME', name='conv_init')(x)\n",
    "        if self.enable_batch_norm:\n",
    "            x = norm(name='bn_init')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, (2, 2), strides=(2, 2), padding='SAME')\n",
    "        for i, block_size in enumerate(self.stage_sizes):\n",
    "            for j in range(block_size):\n",
    "                strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n",
    "                x = self.block_cls(self.num_filters * 2 ** i,\n",
    "                                   strides=strides,\n",
    "                                   conv=conv,\n",
    "                                   norm=norm,\n",
    "                                   act=self.act)(x)\n",
    "        x = jnp.mean(x, axis=(1, 2))\n",
    "        x = nn.Dense(self.num_classes, dtype=self.dtype)(x)\n",
    "        x = jnp.asarray(x, self.dtype)\n",
    "        return x\n",
    "    \n",
    "ResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2], block_cls=ResNetBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ddfba0b-a055-4423-965c-29d9409aee5c"
   },
   "source": [
    "Next, we define the training loop as well as some utilty functions. We will train the network until 100% training accuracy is reached, or for a maximum of ``max_epochs``, to cap the computational cost of this exercise.\n",
    "\n",
    "Your tasks are:\n",
    "1. Implement an exponential learning rate schedule.\n",
    "2. Compute and store training (and validation) loss and accuracy.\n",
    "3. Implement the convergence criterion.\n",
    "4. Train ResNet-18 until the convergence criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrxL_FcK9Nqv"
   },
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_v1PgSww9LWU"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(logits, labels):\n",
    "    \"\"\"Given the model predictions @logits and ground-truth @labels compute\n",
    "       loss and accuracy.\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    * Compute the average cross-entropy loss over a batch, using the unnormalized network\n",
    "       predictions @logits, and the ground-truth @labels.\n",
    "    \n",
    "    * Before computing the cross-entropy loss, the logits should be normalized.\n",
    "      Hint: The optax package allows to apply both softmax and cross-entropy loss\n",
    "      to the logits at the same time.\n",
    "    * Finally, for computing the loss, each label should be 1-hot-encoded. You can use\n",
    "      ``jax.nn.one_hot()``.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError('Task: implement!')\n",
    "    loss = NotImplemented\n",
    "    accuracy = NotImplemented\n",
    "\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1igMHpMaw7x"
   },
   "source": [
    "Test the `compute_metrics` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfoDzhMzaxlz"
   },
   "outputs": [],
   "source": [
    "logits = jnp.array([[1.,2.,0.,-1.],\n",
    "                    [0.,2.,-5.,1.],\n",
    "                    [1.,1.,5.,-1.]])\n",
    "labels = jnp.array([1,\n",
    "                    0,\n",
    "                    2])\n",
    "\n",
    "returned = compute_metrics(logits,labels)\n",
    "expected_mean_sce_loss = np.array(0.9622556)\n",
    "expected_mean_accuracy = np.array(0.6666666)\n",
    "\n",
    "assert returned['loss'].shape == (), \\\n",
    "    'softmax cross-entropy loss is expected here to be averaged over samples'\n",
    "assert returned['accuracy'].shape == (), \\\n",
    "    'accuracy is expected here to be averaged over samples'\n",
    "\n",
    "np.testing.assert_almost_equal(expected_mean_sce_loss, returned['loss'])\n",
    "np.testing.assert_almost_equal(expected_mean_accuracy, returned['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efdysgala8YX"
   },
   "outputs": [],
   "source": [
    "def get_lr_scheduler(base_lr, lr_decay, steps_per_epoch):\n",
    "    \"\"\" Initialize learning rate scheduler\n",
    "    Parameters:\n",
    "        base_lr: initial learning rate\n",
    "        lr_decay: learning rate decay coefficient\n",
    "        steps_per_epoch: number of sgd steps in one training epoch\n",
    "        \n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    Implement an exponential learning rate decay schedule, as proposed by Zhang et al. (2017) to fit\n",
    "    corrupted labels. \n",
    "    \n",
    "    Hint: Look at the ``optax`` documentation for available learning \n",
    "    rate schedulers and how to use them.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError('Task: implement!')\n",
    "    scheduler = NotImplemented\n",
    "\n",
    "    return scheduler\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMUTlAVdbt_H"
   },
   "source": [
    "Test the `get_lr_scheduler` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3AFYP8-buPl"
   },
   "outputs": [],
   "source": [
    "returned = get_lr_scheduler(1e-4, 0.95, 100)\n",
    "assert callable(returned), \"get_lr_scheduler function is expected to return another function that acts on the number of steps.\"\n",
    "expected_lr = np.array(9.943736e-05)\n",
    "np.testing.assert_almost_equal(expected_lr, returned(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cduMAeGdcpHG"
   },
   "outputs": [],
   "source": [
    "def stopping_criterion(accuracy):\n",
    "    \"\"\"Check whether 100% training accuracy is reached\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The method should return True if accuracy is close to 1.0, and False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError('Task: implement!')\n",
    "    is_converged = NotImplemented\n",
    "    \n",
    "    return is_converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utvAkHvMc2Um"
   },
   "source": [
    "Test the `stopping_criterion` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-DKKwDic1--"
   },
   "outputs": [],
   "source": [
    "returned = stopping_criterion(.999)\n",
    "expected_lr = np.array(True)\n",
    "np.testing.assert_equal(expected_lr, returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cd0cb72-7d44-4954-88e3-9ae6eba8aecf"
   },
   "outputs": [],
   "source": [
    "# training utilities\n",
    "def create_train_state(rng):\n",
    "    \"\"\" Initialize network and optimizer\n",
    "    \"\"\"\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "    params = net.init(rng, jnp.ones((1, 32, 32, 3)))['params']\n",
    "    \n",
    "    learning_rate_scheduler = get_lr_scheduler(lr, lr_decay, steps_per_epoch=len(train_loader))\n",
    "    tx = optax.sgd(learning_rate_scheduler, momentum=0.9, nesterov=False)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=net.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for one step\n",
    "    \"\"\"\n",
    "    def compute_loss(params):\n",
    "        \"\"\"Cross-entropy loss with weight decay\n",
    "        \"\"\"\n",
    "        logits = ResNet18().apply({'params': params}, batch['image'])\n",
    "        loss = jnp.mean(\n",
    "            optax.softmax_cross_entropy(logits=logits, labels=jax.nn.one_hot(batch['label'], num_classes)))\n",
    "        weight_penalty_params = jax.tree.leaves(params)\n",
    "        weight_l2 = sum(\n",
    "            [jnp.sum(x ** 2) for x in weight_penalty_params if x.ndim > 1])\n",
    "        weight_penalty = weight_decay * 0.5 * weight_l2\n",
    "        loss = loss + weight_penalty\n",
    "        return loss, logits\n",
    "    grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "    return state, metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    \"\"\"Evaluate ResNet18 on batch using params. \n",
    "    \"\"\"\n",
    "    logits = ResNet18().apply({'params': params}, batch['image'], train=False)\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])\n",
    "\n",
    "\n",
    "def train_epoch(state, train_loader, epoch):\n",
    "    \"\"\"Train for one epoch\n",
    "    \"\"\"\n",
    "    batch_metrics = []\n",
    "    for input, target in train_loader:\n",
    "        batch = {\n",
    "            'image': input,\n",
    "            'label': target,\n",
    "        }\n",
    "\n",
    "        state, train_metrics_ep = train_step(state, batch)\n",
    "        batch_metrics.append(train_metrics_ep)\n",
    "\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "    }\n",
    "\n",
    "    print(\"epoch: {}, train loss: {}, train accuracy: {}\".format(\n",
    "            epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy']))\n",
    "    return state, epoch_metrics_np\n",
    "\n",
    "\n",
    "def eval_model(epoch, params, test_loader):\n",
    "    \"\"\"Evaluate model on test set\n",
    "    \"\"\"\n",
    "    batch_metrics = []\n",
    "    for input, target in test_loader:\n",
    "        batch = {\n",
    "            'image': input,\n",
    "            'label': target,\n",
    "        }\n",
    "\n",
    "        metrics = eval_step(params, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "    }\n",
    "\n",
    "    print(\"epoch: {}, test loss: {}, test accuracy: {}\".format(\n",
    "            epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy']))\n",
    "    return epoch_metrics_np\n",
    "\n",
    "\n",
    "def save_checkpoint(savedir, train_state, epoch):\n",
    "    \"\"\"Save train_state to savedir\"\"\"\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'state': train_state.params,\n",
    "    }\n",
    "    save_path = os.path.join(savedir,\n",
    "                             \"resnet18_\" + str(epoch) + '.pickle')\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(\"Saving model checkpoint to {}.\".format(save_path))\n",
    "\n",
    "def try_cast(maybe_number):\n",
    "    try:\n",
    "        number = int(maybe_number)\n",
    "        return number\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def load_checkpoint(savedir):\n",
    "    \"\"\"\n",
    "        Loads the latest checkpoint from savedir\n",
    "        returns `None` if no checkpoint has been found.\n",
    "    \"\"\"\n",
    "    save_path = glob(checkpoints_dir+'/*.pickle')\n",
    "    path_dict = {}\n",
    "    path_sections = map(\n",
    "        lambda x:x.replace(\".pickle\",\"\").split(\"_\")[-1],\n",
    "        save_path)\n",
    "    for i,maybe_num in enumerate(path_sections):\n",
    "        num = try_cast(maybe_num)\n",
    "        if num is not None:\n",
    "            path_dict[num] = save_path[i]\n",
    "    if len(path_dict) != 0:\n",
    "        latest_checkpoint = sorted(path_dict.items(),\n",
    "                    key=lambda x: x[1])[-1][1]\n",
    "        state_dict = torch.load(latest_checkpoint)\n",
    "        print(\"Loading model from checkpoint {}.\".format(latest_checkpoint))\n",
    "        return state_dict\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_train_state(state_dict):\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "    params = state_dict['state']\n",
    "    start_step = state_dict['epoch']*len(train_loader)\n",
    "    learning_rate_scheduler = get_lr_scheduler(lr, lr_decay, steps_per_epoch=len(train_loader))\n",
    "    def scheduler_with_offset(step):\n",
    "        return learning_rate_scheduler(step + start_step)\n",
    "    tx = optax.sgd(scheduler_with_offset, momentum=0.9, nesterov=False)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=net.apply, params=params, tx=tx)\n",
    "\n",
    "def train_and_evaluate():\n",
    "    \"\"\"Train and evaluate model\"\"\"\n",
    "    # Initialize model\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    state_dict = load_checkpoint(checkpoints_dir)\n",
    "    if state_dict is None:\n",
    "        state = create_train_state(rng)\n",
    "        save_checkpoint(checkpoints_dir, \n",
    "                        state,\n",
    "                        'init')\n",
    "        start_epoch = 0\n",
    "    else:\n",
    "        state = load_train_state(state_dict)\n",
    "        start_epoch = state_dict['epoch']+1\n",
    "\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'loss' : [],\n",
    "            'accuracy': [],\n",
    "        },\n",
    "        'test': {\n",
    "            'loss': [],\n",
    "            'accuracy': [],\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        \n",
    "        state, train_metrics = train_epoch(state, train_loader, epoch)\n",
    "        if stopping_criterion(train_metrics['accuracy']):\n",
    "            print(\"Stopping criterion reached: train accuracy = {}\".format(train_metrics['accuracy']))\n",
    "            break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            test_metrics = eval_model(epoch, state.params, test_loader)\n",
    "            save_checkpoint(checkpoints_dir, state, epoch)\n",
    "            for key in train_metrics:\n",
    "                metrics['train'][key].append(train_metrics[key])\n",
    "                metrics['test'][key].append(test_metrics[key])\n",
    "            \n",
    "    save_checkpoint(checkpoints_dir, state, epoch)\n",
    "    test_metrics = eval_model(epoch, state.params, test_loader)\n",
    "    for key in test_metrics:\n",
    "        metrics['train'][key].append(train_metrics[key])\n",
    "        metrics['test'][key].append(test_metrics[key])\n",
    "    \n",
    "    return state, metrics, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3eeeb9b-60dd-498a-9b2d-e1e5fed0197a"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "trained_state, metrics, last_epoch = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca26b3c3-4790-4f3e-99a2-096b83f9c705"
   },
   "outputs": [],
   "source": [
    "# plot training statistics\n",
    "fig, axs = plt.subplots(3, figsize=(10,10), sharex=True)\n",
    "epochs = np.arange(len(metrics['test']['loss'])) * 10\n",
    "if last_epoch % 10 != 0:\n",
    "    epochs[-1] = last_epoch\n",
    "\n",
    "axs[0].plot(epochs, metrics['train']['loss'], label='train loss')\n",
    "axs[0].plot(epochs, metrics['test']['loss'], label='test loss')\n",
    "axs[0].legend(loc='upper right')\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[1].plot(epochs, metrics['train']['accuracy'], label='train acc')\n",
    "axs[1].plot(epochs, metrics['test']['accuracy'], label='test acc')\n",
    "axs[1].legend(loc='center right')\n",
    "axs[1].set_ylabel('accuracy')\n",
    "axs[1].set_ylim(0., 1.0)\n",
    "\n",
    "# plot learning rate schedule\n",
    "num_batches = len(train_loader)\n",
    "lr_scheduler = get_lr_scheduler(\n",
    "    lr, lr_decay, steps_per_epoch=num_batches)\n",
    "lr_schedule = [jax.device_get(lr_scheduler(epoch * num_batches)).item() for epoch in epochs]\n",
    "\n",
    "axs[2].plot(epochs, lr_schedule)\n",
    "axs[2].set_ylabel('learning rate')\n",
    "axs[2].set_xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec539bdc-5d63-429c-af06-36c2abd586ba"
   },
   "source": [
    "## Explicit regularization\n",
    "\n",
    "If unregularized networks can fit arbitrary labellings of the data, what factors control the learning of patterns that generalize to unseen data? Is the success of deep learning simply explained by the right choice of explicit regularization?\n",
    "\n",
    "The following plot shows the effectiveness of several regularization techniques at contrasting memorization, by comparing the training accuracy reached under several degrees of label noise. Furthermore, for each combination of regularization techniques, the best validation accuracy on clean labels is reported in the legend of the plot.\n",
    "\n",
    "![Comparison of several explict regularization techniques](figs/expl_regularization.png \"Explicit regularization vs label noise\")\n",
    "\n",
    "### Questions\n",
    "\n",
    "* Which technique is most effective at hindering memorization?\n",
    "* Which technique yields the best performance when no corrupted training labels are present?\n",
    "* Is learning only possible with explicit regularization? Motivate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ff8228d-e660-4a73-9f34-81f38de33bc6"
   },
   "source": [
    "## Implicit regularization\n",
    "\n",
    "In this last plot, several regularization techniques are compared on clean labels, against learning without explicit regularization. All networks have been trained until a target training cross-entropy loss value of $0.19$ was reached.\n",
    "\n",
    "![Explicit vs implict regularization](figs/impl_regularization.png \"Explicit vs implicit regularization\")\n",
    "\n",
    "* What do you observe? Is explicit regulaziation needed for achieving non-trivial performance?\n",
    "* If explict regularization alone does not explain learning, could you speculate on what factors, implicit in standard deep learning, are responsible for generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f10d9eb6-5da4-46bb-aead-fdefffee6532"
   },
   "source": [
    "## Next steps\n",
    "The code provided in this notebook is slightly more general than what is required to solve the assignment. In fact, you can use the code to train your networks on clean labels, for instance by also enabling data augmentation and batch normalization. If you choose to do so, you probably need to tweak the learning rate schedule as well. Below, one example of a piece-wise constant learning rate policy, often used in practice, is provided.\n",
    "\n",
    "You can use the schedule proposed to train a VGG-11 network, on clean labels (`label_noise = 0.`), with data augmentation. For such network, it is suggested to use a small weight decay, like $1\\rm{e}-4$ and start with a smaller initial learning rate `lr = 0.01`. Such network will be needed for completing the optional task of Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da759ec4-53b0-4117-96f1-f1127cceb991"
   },
   "outputs": [],
   "source": [
    "num_batches = len(train_loader)\n",
    "step_lr_schedule_args = {30 * num_batches: 0.5, 60 * num_batches: 0.5, 90 * num_batches: 0.5}\n",
    "scheduler = optax.piecewise_constant_schedule(init_value=lr, boundaries_and_scales=step_lr_schedule_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is valuable for us to know how long did it take you to complete this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75f53178-d096-443a-9075-d44f3683a3a3"
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* The data loading boilerplate code and the training algorithm are adapted from the [official JAX documentation](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html).\n",
    "\n",
    "* ResNet definition adapted from the official [FLAX ImageNet example](https://github.com/google/flax/blob/master/examples/imagenet/models.py).\n",
    "\n",
    "* PIL to numpy array transforms are adapted from the following [CIFAR-10 example](https://github.com/hushon/JAX-ResNet-CIFAR10/blob/main/train.py). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2d9a830-6cd7-4a6e-a655-de25648738fe"
   },
   "source": [
    "## References\n",
    "1. [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)  - Zhang et al. ICLR 2017.\n",
    "2. [A Closer Look at Memorization in Deep Networks](https://icml.cc/Conferences/2017/ScheduleMultitrack?event=1327) - Arpit et al. ICML 2017.\n",
    "3. [Sensitivity and Generalization in Neural Networks: an Empirical Study](https://openreview.net/forum?id=HJC2SzZCW) - Novak et al. ICLR 2018.\n",
    "4. [In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning](https://openreview.net/forum?id=6AzZb_7Qo0e) - Neyshabur, Tomioka, and Srebro. ICLR Workshop Track 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5bacea6-7fc2-4bdf-9795-756c2b1d8de3"
   },
   "source": [
    "## Changelog\n",
    "| Version \t| Contribution      \t| Author (Affiliation) \t                | Contact \t        |\n",
    "|---------\t|-------------------\t|-----------------------------------    |---------\t        |\n",
    "| 1.0     \t| First development \t| Matteo Gamba (KTH/EECS/RPL)       \t|  ![contact address](figs/contact.png \"Contact information\") \t|"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "freq_swap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
