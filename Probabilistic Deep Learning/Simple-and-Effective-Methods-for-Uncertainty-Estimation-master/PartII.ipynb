{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Ukfr07CaAE"
   },
   "source": [
    "# Uncertainty Estimation Practical Part II\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVhXsBnNCaAH"
   },
   "source": [
    "# Training Independent Subnetwork for Robust Prediction -- Havasi M et al.[[1]]\n",
    "\n",
    "## Motivation\n",
    "In the first part of this practical, we got hands-on experience working with the state-of-the-art Deep Ensembles(DE) method[[1]]. While this method achieves impressive performance[[2],[3]], it is not without limitations. There are three main issues:\n",
    "* requires more training resources\n",
    "* requires more time to evaluate\n",
    "* requires more memory\n",
    "\n",
    "Many works have tried to tackle one or several of these issues. For example, there are distillation methods that first train an ensemble and teach a single network to mimic the predictions of the ensemble[[4],[5],[6]]. This solves the last two problems, but not the first.\n",
    "\n",
    "Another approach is for the members of the ensemble to share some of their weights with the other members[[1], [7]]. Depending on the amount of sharing, this can drastically reduce all three problems above.\n",
    "\n",
    "In this part, we look at one such weight sharing method called MIMO[[1]].\n",
    "\n",
    "## Abstract\n",
    "Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved `for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.\n",
    "\n",
    "## Method\n",
    "The idea of MIMO is to use a single network and encourage it to learn subnetworks that can be used as an ensemble. This is done by modifying the architecture for multiple inputs and outputs see the figure below(from the paper).\n",
    "![](https://drive.google.com/uc?export=view&id=1E5eU1yGrCyMBQgKWWw-qrXEnJxXlh7ks)\n",
    "\n",
    "During training, a network takes M inputs of potentially different classes as input and outputs M corresponding predictions using multiple heads. The network is optimized with a standard cross entropy loss function matching the output of a head to the class of the corresponding input.\n",
    "\n",
    "At test time, the input is M copies of itself, and the the average of all the heads is the final prediction.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "The following requires implementation:\n",
    "* **Network**: Increase the size of the first and last layers. Convince yourself how the increase in the last layer corresponds to having multiple heads by looking at the provided ``predict_MIMO`` function.\n",
    "+ **Training Step**: Update training code to take M inputs and modify the loss accordingly.\n",
    "+ **Evaluation Step**: Update evaluation code to take M copies as input and average the output of the different heads.\n",
    "\n",
    "\n",
    "[1]:https://arxiv.org/abs/2010.06610\n",
    "[2]:https://arxiv.org/abs/1906.02530v2\n",
    "[3]:https://arxiv.org/abs/1906.01620\n",
    "[4]:https://arxiv.org/abs/1906.05419\n",
    "[5]:https://arxiv.org/abs/2001.04694\n",
    "[6]:https://arxiv.org/abs/1905.00076\n",
    "[7]:https://arxiv.org/abs/2002.06715\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGnUvj3OCaAQ"
   },
   "source": [
    "## Classification on [toy dataset] using MIMO :\n",
    "[toy dataset]: https://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoRNH_GBCaAR"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEA7sFXrCaAS"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.scipy.special import logsumexp\n",
    "import numpy as np\n",
    "from jax.example_libraries import optimizers\n",
    "from functools import partial\n",
    "\n",
    "from jax.config import config\n",
    "# IMPORTANT NOTE:\n",
    "# if you have got a NaN loss and/or have trouble debugging. Then, set\n",
    "# jax_disable_jit to True. This will help you print out the variables.\n",
    "config.update('jax_disable_jit', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqV-pdj_0FCU"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def generate_datasets(N,K,noise):\n",
    "    X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "\n",
    "    for j in range(K):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        r = np.linspace(0.0,1.,N) # radius\n",
    "        t = np.linspace(j*8,(j+1)*8,N) + np.random.randn(N)*noise * (r+1.0) # theta\n",
    "        print(j, np.amin(t), np.amax(t))\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    X = jnp.array(X)\n",
    "    y = jnp.array(y)\n",
    "    y_onehot = one_hot(y, K)\n",
    "    return X, y, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRkaZQoH0lnZ"
   },
   "outputs": [],
   "source": [
    "N = 1000 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, y_onehot = generate_datasets(N, K, noise=0.3)\n",
    "\n",
    "# lets visualize the data:\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Training Data\")\n",
    "print(\"Training X:\", X.shape)\n",
    "print(\"Training y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTgQZRlSCaAU"
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-1):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "def predict_MIMO(params, input):\n",
    "    \"\"\"\n",
    "    The input is of shape (#nets, #examples, #input_dims)=(M,N,D).\n",
    "    while the two outputs are of shape (#nets, #examples, #classes)=(M,N,K).\n",
    "    \"\"\"\n",
    "    def _predict(params, input):\n",
    "        M,D = input.shape\n",
    "        input = input.reshape(M*D)\n",
    "        activations = input\n",
    "        for w, b in params[:-1]:\n",
    "            outputs = jnp.dot(w, activations) + b\n",
    "            activations = jax.nn.relu(outputs)\n",
    "\n",
    "        final_w, final_b = params[-1]\n",
    "        logits = jnp.dot(final_w, activations) + final_b\n",
    "        logits = logits.reshape(M,-1)\n",
    "        probs = jax.nn.softmax(logits, axis=1)\n",
    "        return logits, probs\n",
    "\n",
    "    logits, probs = vmap(_predict, in_axes=(None, 1))(params, input)\n",
    "    logits, probs = jnp.transpose(logits, [1, 0, 2]), jnp.transpose(probs, [1, 0, 2])\n",
    "\n",
    "    return logits, probs\n",
    "\n",
    "@partial(jit, static_argnums=2)\n",
    "def evalMIMO(params, inputs, M):\n",
    "    N, D = inputs.shape\n",
    "    \"\"\"\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The ``inputs`` argument of this function is of shape (#examples, #input_dims)=(N,D).\n",
    "    1. Create a new input of M copies of the input, resulting in shape (M,N,D).\n",
    "    2. Use predict_MIMO with the input from 1 to get logits and probabilities, each of shape (M,N,K).\n",
    "    3. Return a tuple of mean probabilities (shape (N,K)) and all probabilities (shape (M,N,K)).\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    probabilities = NotImplemented\n",
    "    mean_probabilities = NotImplemented\n",
    "    return mean_probabilities, probabilities\n",
    "\n",
    "def batched_logsumexp(logits):\n",
    "    \"\"\"\n",
    "    The input is of shape (M,N,K)=(#nets, #examples, #classes)\n",
    "    The output is of shape (M,N)\n",
    "    \"\"\"\n",
    "    def _logsumexp(logits):\n",
    "        return logsumexp(logits, axis=1)\n",
    "    return vmap(_logsumexp)(logits)\n",
    "\n",
    "def lossMIMO(params, inputs, targets, M, wd, seed):\n",
    "    N, D = inputs.shape\n",
    "    \"\"\"\n",
    "    ==============================\n",
    "    TODO: Implementation required.\n",
    "    ==============================\n",
    "    The ``inputs`` argument of this function is of shape (N,D)=(#examples, #input_dims).\n",
    "    The ``targets` argument of this function is of shape (N,K)=(#examples, #classes).\n",
    "    1. Create a randomly shuffled batch of shape (M,N,D)\n",
    "    2. Use predict_MIMO to get logits and predictions, each of shape (M,N,K)\n",
    "    3. Return the cross entropy loss for the M*N examples (scalar) and weight decay (see Part I)\n",
    "    Tip: tile, take, random.permutation and batched_logsumexp could be handy\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"Task: Implement!\")\n",
    "    loss = NotImplemented\n",
    "    return loss\n",
    "\n",
    "@partial(jit, static_argnums=(4,5))\n",
    "def update_mimo(params, x, y, opt_state, M, wd, seed):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    grads = grad(lossMIMO)(params, x, y, M, wd, seed)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state\n",
    "\n",
    "def logging(params_mimo, X, y, M, log_acc_train, log_nll_train):\n",
    "    mean, _ = evalMIMO(params_mimo, X, M)\n",
    "\n",
    "    y_pred = jnp.argmax(mean, axis=1)\n",
    "    train_acc = jnp.mean(y_pred == y)\n",
    "    y_conf = mean[np.arange(y.shape[0]),y]\n",
    "    train_nll = -jnp.mean(jnp.log(y_conf))\n",
    "\n",
    "    log_acc_train.append(train_acc)\n",
    "    log_nll_train.append(train_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP8_miSer1Zs"
   },
   "outputs": [],
   "source": [
    "M = 4\n",
    "\n",
    "\"\"\"\n",
    "==============================\n",
    "TODO: Implementation required.\n",
    "==============================\n",
    "1. Update size of first layer to handle more inputs.\n",
    "2. Update size of last layer to get multiple heads.\n",
    "\"\"\"\n",
    "raise NotImplementedError(\"Task: Implement!\")\n",
    "\n",
    "layer_sizes = [2, 150, 150, 3]\n",
    "\n",
    "num_epochs = 2000\n",
    "step_size = 0.1\n",
    "wd = 1e-3\n",
    "\n",
    "log_acc, log_nll = [], []\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "params_mimo = init_network_params(layer_sizes, random.PRNGKey(0))\n",
    "opt_state_all = opt_init(params_mimo)\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "for epoch in range(num_epochs):\n",
    "    key, seed = random.split(key)\n",
    "\n",
    "    params_mimo = get_params(opt_state_all)\n",
    "    params_mimo, opt_state_all = update_mimo(params_mimo, X, y_onehot, opt_state_all, M, wd, seed)\n",
    "    logging(params_mimo, X, y, M, log_acc, log_nll)\n",
    "\n",
    "\n",
    "    print('\\r', f'[Epoch {epoch+1}]: Train Acc: {log_acc[-1]:.3f} | Train NLL: {log_nll[-1]:0.3f}', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxBLdOJjqxHy"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
    "fig.tight_layout()\n",
    "axes[0].plot(log_acc)\n",
    "axes[1].plot(log_nll)\n",
    "axes[0].title.set_text('Train Acc')\n",
    "axes[1].title.set_text('Train NLL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UC91rTUe0a1i"
   },
   "outputs": [],
   "source": [
    "# Visualization code\n",
    "def entropy(preds):\n",
    "    preds = np.clip(preds, 1e-7, 1.0)\n",
    "    return -np.sum( preds * np.log(preds), axis=1)\n",
    "\n",
    "def js_terms(distributions):\n",
    "    return entropy(np.mean(distributions, axis=0)), np.mean([entropy(p) for p in distributions], axis=0)\n",
    "\n",
    "def visualize_predictions(X, y, params_list, min=-2.0, max=2.0, res=200, num_nets=1):\n",
    "    xs = np.linspace(min, max, res)\n",
    "    ys = np.linspace(min, max, res)\n",
    "    N, M = len(xs), len(ys)\n",
    "    xy = np.asarray([(_x,_y) for _x in xs for _y in ys])\n",
    "    num_samples = xy.shape[0]\n",
    "\n",
    "    predictions_ensemble, probs = evalMIMO(params_list, xy, num_nets)\n",
    "    predictions = [probs[i,:,:] for i in range(probs.shape[0])]\n",
    "    total, data = js_terms(predictions)\n",
    "\n",
    "    indices = np.unravel_index(np.arange(num_samples), (N,M))\n",
    "\n",
    "    Z, Z2, Z3 = np.zeros((N,M)), np.zeros((N,M)), np.zeros((N,M))\n",
    "    Z[indices] = jnp.argmax(predictions_ensemble, axis=1)\n",
    "    Z2[indices] = total - data\n",
    "    Z3[indices] = data\n",
    "\n",
    "    fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "    axes = axes.flatten()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "    axes[1].contourf(xs, ys, Z.T, cmap=plt.cm.Spectral, levels=50)\n",
    "    axes[2].contourf(xs, ys, Z3.T, cmap='magma',levels=50)\n",
    "    axes[3].contourf(xs, ys, Z2.T, cmap='magma', levels=50)\n",
    "\n",
    "    axes[0].set_xlim([min, max]); axes[0].set_ylim([min, max]);\n",
    "\n",
    "    axes[0].title.set_text('Dataset')\n",
    "    axes[1].title.set_text('Mean')\n",
    "    axes[2].title.set_text('Data Uncertainty')\n",
    "    axes[3].title.set_text('Knowledge Uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlfkaoh8Igjq"
   },
   "outputs": [],
   "source": [
    "visualize_predictions(X, y, params_mimo, num_nets=M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmfZDmK6lq19"
   },
   "source": [
    "where dark to light increases in value.\n",
    "\n",
    "**Discussion points:**\n",
    "* In terms of the discussion points in Part I, is the MIMO ensemble behaving as the Deep Ensemble?\n",
    "* Compared to the Deep Ensemble, how is the entropy of the mean prediction and the diversity of MIMO affected by increasing M?\n",
    "* Finally, it is valuable for us to know how long did it take you to complete this practical?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX512BfVb8T2"
   },
   "source": [
    "## Image Classification (Optional)\n",
    "We provide code in ``classification_mnist_pytorch.py`` for training a single network to classify images of numbers (MNIST). For bonus points, adapt this code to use any of these two ensembling techniques we have learned about in this practical.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "flax",
   "language": "python",
   "name": "flax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
